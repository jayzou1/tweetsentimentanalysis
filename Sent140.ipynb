{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tweet Sentiment Classification\n",
    "\n",
    "This is intended to be an introductory exercise in binary sentiment classification of microblog data using deep learning. We'll be using the Stanford Sentiment140 dataset, which has been a testing benchmark for a multitude of academic papers largely due to its enourmous size. In fact, it's so big that I'm cutting down to a 10% sample for training, as it would otherwise take far too long to train on a personal machine. \n",
    "\n",
    "First things first, we load in and preprocess the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "sent140_full = pd.read_csv(\"./datasets/sentiment140/traindata.csv\", header = None)\n",
    "sent140_test = pd.read_csv(\"./datasets/sentiment140/testdata.csv\", header = None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at what our dataset looks like and compute some summary statistics. Sentiment140 is actually broken into two clasifications: positive (\"4\") and negative (\"0\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810369</td>\n",
       "      <td>Mon Apr 06 22:19:45 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>_TheSpecialOne_</td>\n",
       "      <td>@switchfoot http://twitpic.com/2y1zl - Awww, t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810672</td>\n",
       "      <td>Mon Apr 06 22:19:49 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>scotthamilton</td>\n",
       "      <td>is upset that he can't update his Facebook by ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810917</td>\n",
       "      <td>Mon Apr 06 22:19:53 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>mattycus</td>\n",
       "      <td>@Kenichan I dived many times for the ball. Man...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811184</td>\n",
       "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>ElleCTF</td>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811193</td>\n",
       "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>Karoli</td>\n",
       "      <td>@nationwideclass no, it's not behaving at all....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811372</td>\n",
       "      <td>Mon Apr 06 22:20:00 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>joy_wolf</td>\n",
       "      <td>@Kwesidei not the whole crew</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811592</td>\n",
       "      <td>Mon Apr 06 22:20:03 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>mybirch</td>\n",
       "      <td>Need a hug</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811594</td>\n",
       "      <td>Mon Apr 06 22:20:03 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>coZZ</td>\n",
       "      <td>@LOLTrish hey  long time no see! Yes.. Rains a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811795</td>\n",
       "      <td>Mon Apr 06 22:20:05 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>2Hood4Hollywood</td>\n",
       "      <td>@Tatiana_K nope they didn't have it</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0</td>\n",
       "      <td>1467812025</td>\n",
       "      <td>Mon Apr 06 22:20:09 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>mimismo</td>\n",
       "      <td>@twittera que me muera ?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0</td>\n",
       "      <td>1467812416</td>\n",
       "      <td>Mon Apr 06 22:20:16 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>erinx3leannexo</td>\n",
       "      <td>spring break in plain city... it's snowing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0</td>\n",
       "      <td>1467812579</td>\n",
       "      <td>Mon Apr 06 22:20:17 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>pardonlauren</td>\n",
       "      <td>I just re-pierced my ears</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0</td>\n",
       "      <td>1467812723</td>\n",
       "      <td>Mon Apr 06 22:20:19 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>TLeC</td>\n",
       "      <td>@caregiving I couldn't bear to watch it.  And ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0</td>\n",
       "      <td>1467812771</td>\n",
       "      <td>Mon Apr 06 22:20:19 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>robrobbierobert</td>\n",
       "      <td>@octolinz16 It it counts, idk why I did either...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0</td>\n",
       "      <td>1467812784</td>\n",
       "      <td>Mon Apr 06 22:20:20 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>bayofwolves</td>\n",
       "      <td>@smarrison i would've been the first, but i di...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0</td>\n",
       "      <td>1467812799</td>\n",
       "      <td>Mon Apr 06 22:20:20 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>HairByJess</td>\n",
       "      <td>@iamjazzyfizzle I wish I got to watch it with ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0</td>\n",
       "      <td>1467812964</td>\n",
       "      <td>Mon Apr 06 22:20:22 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>lovesongwriter</td>\n",
       "      <td>Hollis' death scene will hurt me severely to w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0</td>\n",
       "      <td>1467813137</td>\n",
       "      <td>Mon Apr 06 22:20:25 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>armotley</td>\n",
       "      <td>about to file taxes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0</td>\n",
       "      <td>1467813579</td>\n",
       "      <td>Mon Apr 06 22:20:31 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>starkissed</td>\n",
       "      <td>@LettyA ahh ive always wanted to see rent  lov...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0</td>\n",
       "      <td>1467813782</td>\n",
       "      <td>Mon Apr 06 22:20:34 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>gi_gi_bee</td>\n",
       "      <td>@FakerPattyPattz Oh dear. Were you drinking ou...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    0           1                             2         3                4  \\\n",
       "0   0  1467810369  Mon Apr 06 22:19:45 PDT 2009  NO_QUERY  _TheSpecialOne_   \n",
       "1   0  1467810672  Mon Apr 06 22:19:49 PDT 2009  NO_QUERY    scotthamilton   \n",
       "2   0  1467810917  Mon Apr 06 22:19:53 PDT 2009  NO_QUERY         mattycus   \n",
       "3   0  1467811184  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY          ElleCTF   \n",
       "4   0  1467811193  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY           Karoli   \n",
       "5   0  1467811372  Mon Apr 06 22:20:00 PDT 2009  NO_QUERY         joy_wolf   \n",
       "6   0  1467811592  Mon Apr 06 22:20:03 PDT 2009  NO_QUERY          mybirch   \n",
       "7   0  1467811594  Mon Apr 06 22:20:03 PDT 2009  NO_QUERY             coZZ   \n",
       "8   0  1467811795  Mon Apr 06 22:20:05 PDT 2009  NO_QUERY  2Hood4Hollywood   \n",
       "9   0  1467812025  Mon Apr 06 22:20:09 PDT 2009  NO_QUERY          mimismo   \n",
       "10  0  1467812416  Mon Apr 06 22:20:16 PDT 2009  NO_QUERY   erinx3leannexo   \n",
       "11  0  1467812579  Mon Apr 06 22:20:17 PDT 2009  NO_QUERY     pardonlauren   \n",
       "12  0  1467812723  Mon Apr 06 22:20:19 PDT 2009  NO_QUERY             TLeC   \n",
       "13  0  1467812771  Mon Apr 06 22:20:19 PDT 2009  NO_QUERY  robrobbierobert   \n",
       "14  0  1467812784  Mon Apr 06 22:20:20 PDT 2009  NO_QUERY      bayofwolves   \n",
       "15  0  1467812799  Mon Apr 06 22:20:20 PDT 2009  NO_QUERY       HairByJess   \n",
       "16  0  1467812964  Mon Apr 06 22:20:22 PDT 2009  NO_QUERY   lovesongwriter   \n",
       "17  0  1467813137  Mon Apr 06 22:20:25 PDT 2009  NO_QUERY         armotley   \n",
       "18  0  1467813579  Mon Apr 06 22:20:31 PDT 2009  NO_QUERY       starkissed   \n",
       "19  0  1467813782  Mon Apr 06 22:20:34 PDT 2009  NO_QUERY        gi_gi_bee   \n",
       "\n",
       "                                                    5  \n",
       "0   @switchfoot http://twitpic.com/2y1zl - Awww, t...  \n",
       "1   is upset that he can't update his Facebook by ...  \n",
       "2   @Kenichan I dived many times for the ball. Man...  \n",
       "3     my whole body feels itchy and like its on fire   \n",
       "4   @nationwideclass no, it's not behaving at all....  \n",
       "5                       @Kwesidei not the whole crew   \n",
       "6                                         Need a hug   \n",
       "7   @LOLTrish hey  long time no see! Yes.. Rains a...  \n",
       "8                @Tatiana_K nope they didn't have it   \n",
       "9                           @twittera que me muera ?   \n",
       "10        spring break in plain city... it's snowing   \n",
       "11                         I just re-pierced my ears   \n",
       "12  @caregiving I couldn't bear to watch it.  And ...  \n",
       "13  @octolinz16 It it counts, idk why I did either...  \n",
       "14  @smarrison i would've been the first, but i di...  \n",
       "15  @iamjazzyfizzle I wish I got to watch it with ...  \n",
       "16  Hollis' death scene will hurt me severely to w...  \n",
       "17                               about to file taxes   \n",
       "18  @LettyA ahh ive always wanted to see rent  lov...  \n",
       "19  @FakerPattyPattz Oh dear. Were you drinking ou...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent140_full.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Characters per Tweet: 74.09011125\n",
      "Average Words per Tweet: 14.382130625\n",
      "Label Distribution:\n",
      "4    800000\n",
      "0    800000\n",
      "Name: 0, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "print \"Average Characters per Tweet:\", np.average([len(tweet) for tweet in sent140_full[5].values])\n",
    "print \"Average Words per Tweet:\", np.average([len(tweet.split(\" \")) for tweet in sent140_full[5].values])\n",
    "print \"Label Distribution:\"\n",
    "print sent140_full[0].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have approximately 74 characters per tweet (including spaces), 14 words per tweet, and an equal number of positive and negative tweets. We next take a sample of the dataset for use in our neural network training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sent140_train = sent140_full.sample(frac = 0.10)\n",
    "sent140_test = sent140_test[sent140_test[0].isin([4,0])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Embeddings\n",
    "\n",
    "A popular approach in natural language processing is using an embedded representation of words. By embedding words into n-dimensional vectors based on their co-occurence with other words in a corpus, we establish word-level similarities in the matrix that might not be captured in a sparser one-hot vocabulary representation. That is, words that appear in similar semantic circumstances should be embedded into similar vector representations and thus be closer in Euclidean space. We'll be using the Word2Vec library to build word embeddings for this corpus.\n",
    "\n",
    "First, we build a vocabulary mapping for all the words contained within our tweet data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import numpy as np\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "def build_vocab(sentences):\n",
    "    vocab = Counter()\n",
    "    lens = []\n",
    "    for sentence in sentences:\n",
    "        lens.append(len(sentence.split(\" \")))\n",
    "        for word in sentence.split(\" \"):\n",
    "            if word != '':\n",
    "                vocab[word.lower()] += 1\n",
    "\n",
    "    #Summary Statistics\n",
    "    print \"Vocab Size:\", len(vocab)\n",
    "    print \"Number of Words:\", np.sum(vocab.values())\n",
    "    print \"Average Sentence Length:\", np.average(lens)\n",
    "    print \"Top Twenty Most Common:\"\n",
    "    for word in vocab.most_common(20):\n",
    "        print word\n",
    "        \n",
    "    mapping = dict(zip(vocab.keys(), np.arange(len(vocab))))\n",
    "    return vocab, mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab Size: 1193538\n",
      "Number of Words: 21080792\n",
      "Average Sentence Length: 14.382130625\n",
      "Top Twenty Most Common:\n",
      "('i', 746583)\n",
      "('to', 560313)\n",
      "('the', 518728)\n",
      "('a', 376421)\n",
      "('my', 312846)\n",
      "('and', 295670)\n",
      "('you', 237766)\n",
      "('is', 231085)\n",
      "('for', 214050)\n",
      "('in', 209958)\n",
      "('it', 191050)\n",
      "('of', 182598)\n",
      "('on', 161592)\n",
      "('so', 145769)\n",
      "('have', 143295)\n",
      "('that', 128774)\n",
      "('me', 128489)\n",
      "(\"i'm\", 127616)\n",
      "('but', 124673)\n",
      "('just', 124453)\n"
     ]
    }
   ],
   "source": [
    "sent140s = sent140_train[5].values\n",
    "sent140t = sent140_test[5].values\n",
    "sent140_vocab, sent140_mapping = build_vocab(sent140_full[5].values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating embedded representations of sentences\n",
    "\n",
    "Here we generate the embedded vector representations of each sentence, using an \"out-of-vocabulary\" token for each word that appears less than 5 times in the corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def preprocess_sents(sents, vocab, mincount):\n",
    "    p_sents = []\n",
    "    for sent in sents:\n",
    "        processed = []\n",
    "        for word in sent.split(\" \"):\n",
    "            if vocab[word.lower().strip()] >= mincount:\n",
    "                processed.append(word.lower())\n",
    "            else:\n",
    "                processed.append(\"oov\")\n",
    "        p_sents.append(processed)\n",
    "        \n",
    "    return p_sents\n",
    "\n",
    "def generate_vectors(sentences, df_labels, mapping):\n",
    "    vectors = []\n",
    "    labels = []\n",
    "    maximum = 0\n",
    "    for sentence, label in zip(sentences, df_labels):\n",
    "        if len(sentence) > 140:\n",
    "            continue\n",
    "        vector = [mapping[word.lower()] for word in sentence.split(\" \")]\n",
    "        vectors.append(vector)\n",
    "        labels.append(label)\n",
    "    return vectors, labels\n",
    "\n",
    "def generate_embedv(sentences, vocab, embed):\n",
    "    vectors = []\n",
    "    oovcount = 0\n",
    "    for sentence in sentences:\n",
    "        vector = []\n",
    "        for word in sentence:\n",
    "            if word in embed:\n",
    "                vector.append(embed[word])\n",
    "            else:\n",
    "                vector.append(embed[\"oov\"])\n",
    "                oovcount += 1\n",
    "        vectors.append(np.array(vector))\n",
    "    print oovcount\n",
    "    return vectors\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mincount = 5\n",
    "embedding_size = 30\n",
    "\n",
    "processed_train = preprocess_sents(sent140s, sent140_vocab, mincount)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Jay/anaconda2/lib/python2.7/site-packages/ipykernel_launcher.py:32: DeprecationWarning: Call to deprecated `__contains__` (Method will be removed in 4.0.0, use self.wv.__contains__() instead).\n",
      "/Users/Jay/anaconda2/lib/python2.7/site-packages/ipykernel_launcher.py:33: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "/Users/Jay/anaconda2/lib/python2.7/site-packages/ipykernel_launcher.py:35: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "111195\n",
      "388\n"
     ]
    }
   ],
   "source": [
    "embedding_train = Word2Vec(processed_train, size=embedding_size, min_count = mincount)\n",
    "train_embeds = generate_embedv(processed_train, sent140_vocab, embedding_train)\n",
    "processed_test = preprocess_sents(sent140t, sent140_vocab, mincount)\n",
    "test_embeds = generate_embedv(processed_test, sent140_vocab, embedding_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning a Neural Network\n",
    "\n",
    "We'll be using Keras to create a simple neural network to learn on and classify our embedded sentence representations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os, errno\n",
    "import pandas as pd\n",
    "import sys\n",
    "from keras.callbacks import EarlyStopping, Callback\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Activation, Dense, LSTM, Flatten, Dropout, Conv1D, MaxPooling1D, SimpleRNN, GRU, Bidirectional\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras import optimizers\n",
    "from keras import regularizers\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Padding\n",
    "\n",
    "Since neural networks generally have a fixed input size and our embedded sentences vary in length, we have to pad each sequence so that they are all the same size. We need to make sure that we can predict on our test data, so we pad our data with zeros to be the same size as the maximum length vector of either training or test data, whichever is larger."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "max_train = np.max([len(arr) for arr in train_embeds])\n",
    "max_test = np.max([len(arr) for arr in test_embeds])\n",
    "\n",
    "max_len = max_train\n",
    "if max_test > max_len:\n",
    "    max_len = max_test\n",
    "\n",
    "padded_train = pad_sequences(train_embeds, dtype = \"float32\", maxlen = max_len)\n",
    "padded_test = pad_sequences(test_embeds, dtype = \"float32\", maxlen = max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "labels_train = sent140_train[0].map({4: 1, 0:0}).values\n",
    "labels_test = sent140_test[0].map({4: 1, 0:0}).values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checkpointing\n",
    "\n",
    "At this point it'd be nice to save our processed data, so we'll output the padded training and test data alongside our embedding model to files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "embedding_train.save(\"trained_embeddings.wv\")\n",
    "np.save(\"vocab.npy\", sent140_vocab)\n",
    "np.savez(\"train.npz\", padded_train, labels_train)\n",
    "np.savez(\"test.npz\", padded_test, labels_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we want to load our preprocessed data..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "embedding_train = Word2Vec.load(\"trained_embeddings.wv\")\n",
    "sent140_vocab = np.load(\"vocab.npy\").item()\n",
    "train_data = np.load(\"train.npz\")\n",
    "test_data = np.load(\"test.npz\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network Architecture\n",
    "\n",
    "For the purposes of this exercise, we'll use a 1-dimensional convolutional network.[Dos Snatos and Gatti, 2014] (https://www.aclweb.org/anthology/C14-1008) was able to achieve about 87% accuracy on the same dataset using a convolutional network, although they also utilized character embeddings alongside word embeddings. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def shuffle_together(a, b):\n",
    "    rng_state = np.random.get_state()\n",
    "    np.random.shuffle(a)\n",
    "    np.random.set_state(rng_state)\n",
    "    np.random.shuffle(b)\n",
    "\n",
    "def split_data(data, labels):\n",
    "    shuffle_together(data, labels)\n",
    "    X_train, X_val, y_train, y_val = train_test_split(data, labels, test_size = 0.2)\n",
    "    return X_train, X_val, y_train, y_val\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 10000\n",
    "optimizer = optimizers.rmsprop(lr = 0.001)\n",
    "epochs = 50\n",
    "\n",
    "class History(Callback):\n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.losses = []\n",
    "        self.train_acc = []\n",
    "        self.val_acc = []\n",
    "\n",
    "    def on_epoch_end(self, batch, logs={}):\n",
    "        self.losses.append(logs.get('loss'))\n",
    "        self.train_acc.append(logs.get('acc'))\n",
    "        self.val_acc.append(logs.get('val_acc'))\n",
    "\n",
    "def train_model(layer_func, train_x, val_x, train_y, val_y, epochs):\n",
    "    input_size = (train_x.shape[1], train_x.shape[2])\n",
    "    model = layer_func(input_size)\n",
    "    model.compile(loss = \"binary_crossentropy\", optimizer = optimizer, metrics = [\"accuracy\"])\n",
    "    history = History()\n",
    "    earlystop = EarlyStopping(monitor='val_acc', min_delta=0.001, patience=5, verbose=1, mode='auto')\n",
    "    model.fit(train_x, train_y, validation_data = (val_x, val_y), shuffle = True, batch_size = batch_size, epochs = epochs, callbacks = [history, earlystop])\n",
    "    return model, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras import regularizers\n",
    "\n",
    "def cnn(input_size):\n",
    "    model = Sequential()\n",
    "    model.add(Conv1D(filters = 16, kernel_size = 4,input_shape = (input_size)))\n",
    "    model.add(MaxPooling1D(pool_size=2, padding='valid'))\n",
    "#     model.add(Dropout(0.5))\n",
    "    model.add(Conv1D(filters = 16, kernel_size = 4,input_shape = (input_size)))\n",
    "    model.add(MaxPooling1D(pool_size=2, padding='valid'))\n",
    "#     model.add(Dropout(0.5))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(1, activation = \"sigmoid\"))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 115199 samples, validate on 28800 samples\n",
      "Epoch 1/50\n",
      "115199/115199 [==============================] - 9s - loss: 0.6961 - acc: 0.5535 - val_loss: 0.6725 - val_acc: 0.5907\n",
      "Epoch 2/50\n",
      "115199/115199 [==============================] - 11s - loss: 0.6565 - acc: 0.6201 - val_loss: 0.6446 - val_acc: 0.6325\n",
      "Epoch 3/50\n",
      "115199/115199 [==============================] - 13s - loss: 0.6351 - acc: 0.6452 - val_loss: 0.6319 - val_acc: 0.6474\n",
      "Epoch 4/50\n",
      "115199/115199 [==============================] - 16s - loss: 0.6223 - acc: 0.6587 - val_loss: 0.6179 - val_acc: 0.6636\n",
      "Epoch 5/50\n",
      "115199/115199 [==============================] - 17s - loss: 0.6152 - acc: 0.6663 - val_loss: 0.6124 - val_acc: 0.6690\n",
      "Epoch 6/50\n",
      "115199/115199 [==============================] - 13s - loss: 0.6083 - acc: 0.6733 - val_loss: 0.6068 - val_acc: 0.6764\n",
      "Epoch 7/50\n",
      "115199/115199 [==============================] - 14s - loss: 0.6037 - acc: 0.6781 - val_loss: 0.6041 - val_acc: 0.6775\n",
      "Epoch 8/50\n",
      "115199/115199 [==============================] - 19s - loss: 0.5993 - acc: 0.6811 - val_loss: 0.6017 - val_acc: 0.6820\n",
      "Epoch 9/50\n",
      "115199/115199 [==============================] - 18s - loss: 0.5970 - acc: 0.6841 - val_loss: 0.5967 - val_acc: 0.6864\n",
      "Epoch 10/50\n",
      "115199/115199 [==============================] - 17s - loss: 0.5933 - acc: 0.6864 - val_loss: 0.5956 - val_acc: 0.6881\n",
      "Epoch 11/50\n",
      "115199/115199 [==============================] - 16s - loss: 0.5910 - acc: 0.6893 - val_loss: 0.5954 - val_acc: 0.6883\n",
      "Epoch 12/50\n",
      "115199/115199 [==============================] - 20s - loss: 0.5888 - acc: 0.6908 - val_loss: 0.5905 - val_acc: 0.6898\n",
      "Epoch 13/50\n",
      "115199/115199 [==============================] - 15s - loss: 0.5864 - acc: 0.6932 - val_loss: 0.5888 - val_acc: 0.6914\n",
      "Epoch 14/50\n",
      "115199/115199 [==============================] - 14s - loss: 0.5835 - acc: 0.6955 - val_loss: 0.5869 - val_acc: 0.6935\n",
      "Epoch 15/50\n",
      "115199/115199 [==============================] - 12s - loss: 0.5831 - acc: 0.6959 - val_loss: 0.5861 - val_acc: 0.6932\n",
      "Epoch 16/50\n",
      "115199/115199 [==============================] - 11s - loss: 0.5787 - acc: 0.6998 - val_loss: 0.5869 - val_acc: 0.6941\n",
      "Epoch 17/50\n",
      "115199/115199 [==============================] - 11s - loss: 0.5793 - acc: 0.6993 - val_loss: 0.5821 - val_acc: 0.6972\n",
      "Epoch 18/50\n",
      "115199/115199 [==============================] - 11s - loss: 0.5766 - acc: 0.7013 - val_loss: 0.5812 - val_acc: 0.6976\n",
      "Epoch 19/50\n",
      "115199/115199 [==============================] - 14s - loss: 0.5757 - acc: 0.7021 - val_loss: 0.5814 - val_acc: 0.6988\n",
      "Epoch 20/50\n",
      "115199/115199 [==============================] - 13s - loss: 0.5732 - acc: 0.7035 - val_loss: 0.5806 - val_acc: 0.7005\n",
      "Epoch 21/50\n",
      "115199/115199 [==============================] - 13s - loss: 0.5721 - acc: 0.7035 - val_loss: 0.5770 - val_acc: 0.7007\n",
      "Epoch 22/50\n",
      "115199/115199 [==============================] - 13s - loss: 0.5714 - acc: 0.7053 - val_loss: 0.5785 - val_acc: 0.7011\n",
      "Epoch 23/50\n",
      "115199/115199 [==============================] - 15s - loss: 0.5676 - acc: 0.7078 - val_loss: 0.5777 - val_acc: 0.6989\n",
      "Epoch 24/50\n",
      "115199/115199 [==============================] - 13s - loss: 0.5691 - acc: 0.7072 - val_loss: 0.5737 - val_acc: 0.7028\n",
      "Epoch 25/50\n",
      "115199/115199 [==============================] - 12s - loss: 0.5677 - acc: 0.7078 - val_loss: 0.5732 - val_acc: 0.7030\n",
      "Epoch 26/50\n",
      "115199/115199 [==============================] - 15s - loss: 0.5658 - acc: 0.7096 - val_loss: 0.5718 - val_acc: 0.7050\n",
      "Epoch 27/50\n",
      "115199/115199 [==============================] - 14s - loss: 0.5644 - acc: 0.7111 - val_loss: 0.5707 - val_acc: 0.7056\n",
      "Epoch 28/50\n",
      "115199/115199 [==============================] - 17s - loss: 0.5642 - acc: 0.7110 - val_loss: 0.5706 - val_acc: 0.7057\n",
      "Epoch 29/50\n",
      "115199/115199 [==============================] - 18s - loss: 0.5617 - acc: 0.7132 - val_loss: 0.5708 - val_acc: 0.7050\n",
      "Epoch 30/50\n",
      "115199/115199 [==============================] - 16s - loss: 0.5618 - acc: 0.7131 - val_loss: 0.5724 - val_acc: 0.7028\n",
      "Epoch 31/50\n",
      "115199/115199 [==============================] - 18s - loss: 0.5602 - acc: 0.7142 - val_loss: 0.5735 - val_acc: 0.7018\n",
      "Epoch 32/50\n",
      "115199/115199 [==============================] - 16s - loss: 0.5592 - acc: 0.7153 - val_loss: 0.5684 - val_acc: 0.7069\n",
      "Epoch 33/50\n",
      "115199/115199 [==============================] - 15s - loss: 0.5590 - acc: 0.7147 - val_loss: 0.5666 - val_acc: 0.7088\n",
      "Epoch 34/50\n",
      "115199/115199 [==============================] - 12s - loss: 0.5594 - acc: 0.7144 - val_loss: 0.5655 - val_acc: 0.7088\n",
      "Epoch 35/50\n",
      "115199/115199 [==============================] - 12s - loss: 0.5578 - acc: 0.7155 - val_loss: 0.5683 - val_acc: 0.7061\n",
      "Epoch 36/50\n",
      "115199/115199 [==============================] - 11s - loss: 0.5563 - acc: 0.7176 - val_loss: 0.5664 - val_acc: 0.7089\n",
      "Epoch 37/50\n",
      "115199/115199 [==============================] - 12s - loss: 0.5567 - acc: 0.7170 - val_loss: 0.5650 - val_acc: 0.7094\n",
      "Epoch 38/50\n",
      "115199/115199 [==============================] - 11s - loss: 0.5552 - acc: 0.7184 - val_loss: 0.5628 - val_acc: 0.7093\n",
      "Epoch 39/50\n",
      "115199/115199 [==============================] - 11s - loss: 0.5549 - acc: 0.7177 - val_loss: 0.5627 - val_acc: 0.7107\n",
      "Epoch 40/50\n",
      "115199/115199 [==============================] - 11s - loss: 0.5528 - acc: 0.7200 - val_loss: 0.5622 - val_acc: 0.7109\n",
      "Epoch 41/50\n",
      "115199/115199 [==============================] - 13s - loss: 0.5536 - acc: 0.7201 - val_loss: 0.5657 - val_acc: 0.7100\n",
      "Epoch 42/50\n",
      "115199/115199 [==============================] - 12s - loss: 0.5516 - acc: 0.7206 - val_loss: 0.5637 - val_acc: 0.7076\n",
      "Epoch 43/50\n",
      "115199/115199 [==============================] - 11s - loss: 0.5543 - acc: 0.7186 - val_loss: 0.5602 - val_acc: 0.7111\n",
      "Epoch 44/50\n",
      "115199/115199 [==============================] - 12s - loss: 0.5521 - acc: 0.7201 - val_loss: 0.5604 - val_acc: 0.7125\n",
      "Epoch 45/50\n",
      "115199/115199 [==============================] - 12s - loss: 0.5514 - acc: 0.7209 - val_loss: 0.5596 - val_acc: 0.7127\n",
      "Epoch 46/50\n",
      "115199/115199 [==============================] - 11s - loss: 0.5507 - acc: 0.7210 - val_loss: 0.5661 - val_acc: 0.7049\n",
      "Epoch 47/50\n",
      "115199/115199 [==============================] - 13s - loss: 0.5501 - acc: 0.7213 - val_loss: 0.5606 - val_acc: 0.7101\n",
      "Epoch 48/50\n",
      "115199/115199 [==============================] - 12s - loss: 0.5486 - acc: 0.7223 - val_loss: 0.5625 - val_acc: 0.7083\n",
      "Epoch 49/50\n",
      "115199/115199 [==============================] - 12s - loss: 0.5500 - acc: 0.7216 - val_loss: 0.5578 - val_acc: 0.7134\n",
      "Epoch 50/50\n",
      "115199/115199 [==============================] - 12s - loss: 0.5490 - acc: 0.7219 - val_loss: 0.5600 - val_acc: 0.7128\n",
      "Epoch 00049: early stopping\n",
      "16001/16001 [==============================] - 1s     \n",
      "Train on 115199 samples, validate on 28800 samples\n",
      "Epoch 1/50\n",
      "115199/115199 [==============================] - 13s - loss: 0.7036 - acc: 0.5414 - val_loss: 0.6725 - val_acc: 0.5974\n",
      "Epoch 2/50\n",
      "115199/115199 [==============================] - 18s - loss: 0.6595 - acc: 0.6152 - val_loss: 0.6442 - val_acc: 0.6336\n",
      "Epoch 3/50\n",
      "115199/115199 [==============================] - 13s - loss: 0.6359 - acc: 0.6437 - val_loss: 0.6328 - val_acc: 0.6461\n",
      "Epoch 4/50\n",
      "115199/115199 [==============================] - 12s - loss: 0.6231 - acc: 0.6573 - val_loss: 0.6222 - val_acc: 0.6598\n",
      "Epoch 5/50\n",
      "115199/115199 [==============================] - 12s - loss: 0.6142 - acc: 0.6667 - val_loss: 0.6152 - val_acc: 0.6665\n",
      "Epoch 6/50\n",
      "115199/115199 [==============================] - 12s - loss: 0.6104 - acc: 0.6718 - val_loss: 0.6104 - val_acc: 0.6703\n",
      "Epoch 7/50\n",
      "115199/115199 [==============================] - 12s - loss: 0.6048 - acc: 0.6759 - val_loss: 0.6069 - val_acc: 0.6749\n",
      "Epoch 8/50\n",
      "115199/115199 [==============================] - 12s - loss: 0.6022 - acc: 0.6788 - val_loss: 0.6050 - val_acc: 0.6771\n",
      "Epoch 9/50\n",
      "115199/115199 [==============================] - 12s - loss: 0.5979 - acc: 0.6826 - val_loss: 0.6023 - val_acc: 0.6789\n",
      "Epoch 10/50\n",
      "115199/115199 [==============================] - 12s - loss: 0.5957 - acc: 0.6848 - val_loss: 0.6023 - val_acc: 0.6795\n",
      "Epoch 11/50\n",
      "115199/115199 [==============================] - 12s - loss: 0.5918 - acc: 0.6889 - val_loss: 0.5970 - val_acc: 0.6813\n",
      "Epoch 12/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "115199/115199 [==============================] - 12s - loss: 0.5914 - acc: 0.6883 - val_loss: 0.5965 - val_acc: 0.6819\n",
      "Epoch 13/50\n",
      "115199/115199 [==============================] - 13s - loss: 0.5888 - acc: 0.6903 - val_loss: 0.5993 - val_acc: 0.6807\n",
      "Epoch 14/50\n",
      "115199/115199 [==============================] - 12s - loss: 0.5858 - acc: 0.6932 - val_loss: 0.5921 - val_acc: 0.6872\n",
      "Epoch 15/50\n",
      "115199/115199 [==============================] - 12s - loss: 0.5850 - acc: 0.6947 - val_loss: 0.5971 - val_acc: 0.6858\n",
      "Epoch 16/50\n",
      "115199/115199 [==============================] - 12s - loss: 0.5816 - acc: 0.6976 - val_loss: 0.5883 - val_acc: 0.6903\n",
      "Epoch 17/50\n",
      "115199/115199 [==============================] - 12s - loss: 0.5817 - acc: 0.6962 - val_loss: 0.5889 - val_acc: 0.6894\n",
      "Epoch 18/50\n",
      "115199/115199 [==============================] - 12s - loss: 0.5803 - acc: 0.6983 - val_loss: 0.5858 - val_acc: 0.6918\n",
      "Epoch 19/50\n",
      "115199/115199 [==============================] - 12s - loss: 0.5781 - acc: 0.6997 - val_loss: 0.5867 - val_acc: 0.6912\n",
      "Epoch 20/50\n",
      "115199/115199 [==============================] - 12s - loss: 0.5764 - acc: 0.7008 - val_loss: 0.5835 - val_acc: 0.6925\n",
      "Epoch 21/50\n",
      "115199/115199 [==============================] - 12s - loss: 0.5757 - acc: 0.7029 - val_loss: 0.5835 - val_acc: 0.6939\n",
      "Epoch 22/50\n",
      "115199/115199 [==============================] - 12s - loss: 0.5732 - acc: 0.7036 - val_loss: 0.5850 - val_acc: 0.6928\n",
      "Epoch 23/50\n",
      "115199/115199 [==============================] - 12s - loss: 0.5735 - acc: 0.7025 - val_loss: 0.5821 - val_acc: 0.6950\n",
      "Epoch 24/50\n",
      "115199/115199 [==============================] - 12s - loss: 0.5712 - acc: 0.7054 - val_loss: 0.5876 - val_acc: 0.6918\n",
      "Epoch 25/50\n",
      "115199/115199 [==============================] - 12s - loss: 0.5706 - acc: 0.7059 - val_loss: 0.5796 - val_acc: 0.6980\n",
      "Epoch 26/50\n",
      "115199/115199 [==============================] - 12s - loss: 0.5696 - acc: 0.7060 - val_loss: 0.5785 - val_acc: 0.6987\n",
      "Epoch 27/50\n",
      "115199/115199 [==============================] - 12s - loss: 0.5684 - acc: 0.7072 - val_loss: 0.5813 - val_acc: 0.6980\n",
      "Epoch 28/50\n",
      "115199/115199 [==============================] - 12s - loss: 0.5676 - acc: 0.7080 - val_loss: 0.5765 - val_acc: 0.6989\n",
      "Epoch 29/50\n",
      "115199/115199 [==============================] - 11s - loss: 0.5675 - acc: 0.7077 - val_loss: 0.5749 - val_acc: 0.7005\n",
      "Epoch 30/50\n",
      "115199/115199 [==============================] - 12s - loss: 0.5639 - acc: 0.7105 - val_loss: 0.5770 - val_acc: 0.7003\n",
      "Epoch 31/50\n",
      "115199/115199 [==============================] - 12s - loss: 0.5654 - acc: 0.7092 - val_loss: 0.5724 - val_acc: 0.7019\n",
      "Epoch 32/50\n",
      "115199/115199 [==============================] - 12s - loss: 0.5638 - acc: 0.7112 - val_loss: 0.5716 - val_acc: 0.7022\n",
      "Epoch 33/50\n",
      "115199/115199 [==============================] - 12s - loss: 0.5624 - acc: 0.7116 - val_loss: 0.5745 - val_acc: 0.7024\n",
      "Epoch 34/50\n",
      "115199/115199 [==============================] - 12s - loss: 0.5614 - acc: 0.7112 - val_loss: 0.5759 - val_acc: 0.7030\n",
      "Epoch 35/50\n",
      "115199/115199 [==============================] - 12s - loss: 0.5616 - acc: 0.7125 - val_loss: 0.5723 - val_acc: 0.7040\n",
      "Epoch 36/50\n",
      "115199/115199 [==============================] - 12s - loss: 0.5614 - acc: 0.7119 - val_loss: 0.5686 - val_acc: 0.7064\n",
      "Epoch 37/50\n",
      "115199/115199 [==============================] - 12s - loss: 0.5602 - acc: 0.7128 - val_loss: 0.5679 - val_acc: 0.7066\n",
      "Epoch 38/50\n",
      "115199/115199 [==============================] - 19s - loss: 0.5588 - acc: 0.7142 - val_loss: 0.5693 - val_acc: 0.7057\n",
      "Epoch 39/50\n",
      "115199/115199 [==============================] - 13s - loss: 0.5569 - acc: 0.7159 - val_loss: 0.5702 - val_acc: 0.7057\n",
      "Epoch 40/50\n",
      "115199/115199 [==============================] - 13s - loss: 0.5584 - acc: 0.7143 - val_loss: 0.5664 - val_acc: 0.7076\n",
      "Epoch 41/50\n",
      "115199/115199 [==============================] - 12s - loss: 0.5576 - acc: 0.7150 - val_loss: 0.5656 - val_acc: 0.7077\n",
      "Epoch 42/50\n",
      "115199/115199 [==============================] - 11s - loss: 0.5565 - acc: 0.7150 - val_loss: 0.5669 - val_acc: 0.7065\n",
      "Epoch 43/50\n",
      "115199/115199 [==============================] - 14s - loss: 0.5549 - acc: 0.7170 - val_loss: 0.5692 - val_acc: 0.7069\n",
      "Epoch 44/50\n",
      "115199/115199 [==============================] - 16s - loss: 0.5549 - acc: 0.7170 - val_loss: 0.5725 - val_acc: 0.7057\n",
      "Epoch 45/50\n",
      "115199/115199 [==============================] - 15s - loss: 0.5542 - acc: 0.7173 - val_loss: 0.5687 - val_acc: 0.7066\n",
      "Epoch 46/50\n",
      "115199/115199 [==============================] - 14s - loss: 0.5521 - acc: 0.7194 - val_loss: 0.5645 - val_acc: 0.7089\n",
      "Epoch 47/50\n",
      "115199/115199 [==============================] - 14s - loss: 0.5540 - acc: 0.7169 - val_loss: 0.5661 - val_acc: 0.7091\n",
      "Epoch 48/50\n",
      "115199/115199 [==============================] - 12s - loss: 0.5538 - acc: 0.7178 - val_loss: 0.5642 - val_acc: 0.7102\n",
      "Epoch 49/50\n",
      "115199/115199 [==============================] - 12s - loss: 0.5495 - acc: 0.7213 - val_loss: 0.5694 - val_acc: 0.7051\n",
      "Epoch 50/50\n",
      "115199/115199 [==============================] - 12s - loss: 0.5521 - acc: 0.7184 - val_loss: 0.5657 - val_acc: 0.7084\n",
      "15648/16001 [============================>.] - ETA: 0sTrain on 115199 samples, validate on 28800 samples\n",
      "Epoch 1/50\n",
      "115199/115199 [==============================] - 11s - loss: 0.7318 - acc: 0.5021 - val_loss: 0.6924 - val_acc: 0.5514\n",
      "Epoch 2/50\n",
      "115199/115199 [==============================] - 12s - loss: 0.6732 - acc: 0.5889 - val_loss: 0.6532 - val_acc: 0.6260\n",
      "Epoch 3/50\n",
      "115199/115199 [==============================] - 12s - loss: 0.6407 - acc: 0.6370 - val_loss: 0.6312 - val_acc: 0.6511\n",
      "Epoch 4/50\n",
      "115199/115199 [==============================] - 12s - loss: 0.6234 - acc: 0.6559 - val_loss: 0.6225 - val_acc: 0.6582\n",
      "Epoch 5/50\n",
      "115199/115199 [==============================] - 12s - loss: 0.6132 - acc: 0.6666 - val_loss: 0.6216 - val_acc: 0.6569\n",
      "Epoch 6/50\n",
      "115199/115199 [==============================] - 12s - loss: 0.6081 - acc: 0.6722 - val_loss: 0.6084 - val_acc: 0.6739\n",
      "Epoch 7/50\n",
      "115199/115199 [==============================] - 12s - loss: 0.6040 - acc: 0.6766 - val_loss: 0.6056 - val_acc: 0.6768\n",
      "Epoch 8/50\n",
      "115199/115199 [==============================] - 12s - loss: 0.6001 - acc: 0.6792 - val_loss: 0.6036 - val_acc: 0.6789\n",
      "Epoch 9/50\n",
      "115199/115199 [==============================] - 12s - loss: 0.5956 - acc: 0.6842 - val_loss: 0.6012 - val_acc: 0.6802\n",
      "Epoch 10/50\n",
      "115199/115199 [==============================] - 12s - loss: 0.5927 - acc: 0.6862 - val_loss: 0.6029 - val_acc: 0.6799\n",
      "Epoch 11/50\n",
      "115199/115199 [==============================] - 12s - loss: 0.5908 - acc: 0.6878 - val_loss: 0.5977 - val_acc: 0.6797\n",
      "Epoch 12/50\n",
      "115199/115199 [==============================] - 12s - loss: 0.5874 - acc: 0.6918 - val_loss: 0.5947 - val_acc: 0.6866\n",
      "Epoch 13/50\n",
      "115199/115199 [==============================] - 12s - loss: 0.5873 - acc: 0.6919 - val_loss: 0.5914 - val_acc: 0.6886\n",
      "Epoch 14/50\n",
      "115199/115199 [==============================] - 12s - loss: 0.5850 - acc: 0.6941 - val_loss: 0.5892 - val_acc: 0.6881\n",
      "Epoch 15/50\n",
      "115199/115199 [==============================] - 12s - loss: 0.5827 - acc: 0.6953 - val_loss: 0.5878 - val_acc: 0.6894\n",
      "Epoch 16/50\n",
      "115199/115199 [==============================] - 12s - loss: 0.5825 - acc: 0.6966 - val_loss: 0.5872 - val_acc: 0.6913\n",
      "Epoch 17/50\n",
      "115199/115199 [==============================] - 12s - loss: 0.5794 - acc: 0.6983 - val_loss: 0.5855 - val_acc: 0.6910\n",
      "Epoch 18/50\n",
      "115199/115199 [==============================] - 12s - loss: 0.5793 - acc: 0.6997 - val_loss: 0.5856 - val_acc: 0.6905\n",
      "Epoch 19/50\n",
      "115199/115199 [==============================] - 12s - loss: 0.5766 - acc: 0.7014 - val_loss: 0.5866 - val_acc: 0.6897\n",
      "Epoch 20/50\n",
      "115199/115199 [==============================] - 12s - loss: 0.5765 - acc: 0.7006 - val_loss: 0.5857 - val_acc: 0.6945\n",
      "Epoch 21/50\n",
      "115199/115199 [==============================] - 12s - loss: 0.5746 - acc: 0.7032 - val_loss: 0.5814 - val_acc: 0.6955\n",
      "Epoch 22/50\n",
      "115199/115199 [==============================] - 12s - loss: 0.5738 - acc: 0.7042 - val_loss: 0.5828 - val_acc: 0.6966\n",
      "Epoch 23/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "115199/115199 [==============================] - 12s - loss: 0.5724 - acc: 0.7044 - val_loss: 0.5885 - val_acc: 0.6931\n",
      "Epoch 24/50\n",
      "115199/115199 [==============================] - 12s - loss: 0.5716 - acc: 0.7045 - val_loss: 0.5809 - val_acc: 0.6935\n",
      "Epoch 25/50\n",
      "115199/115199 [==============================] - 14s - loss: 0.5712 - acc: 0.7059 - val_loss: 0.5785 - val_acc: 0.6969\n",
      "Epoch 26/50\n",
      "115199/115199 [==============================] - 13s - loss: 0.5690 - acc: 0.7069 - val_loss: 0.5819 - val_acc: 0.6935\n",
      "Epoch 27/50\n",
      "115199/115199 [==============================] - 11s - loss: 0.5694 - acc: 0.7070 - val_loss: 0.5781 - val_acc: 0.6953\n",
      "Epoch 28/50\n",
      "115199/115199 [==============================] - 11s - loss: 0.5684 - acc: 0.7079 - val_loss: 0.5758 - val_acc: 0.7003\n",
      "Epoch 29/50\n",
      "115199/115199 [==============================] - 11s - loss: 0.5680 - acc: 0.7078 - val_loss: 0.5757 - val_acc: 0.7000\n",
      "Epoch 30/50\n",
      "115199/115199 [==============================] - 11s - loss: 0.5654 - acc: 0.7100 - val_loss: 0.5754 - val_acc: 0.6986\n",
      "Epoch 31/50\n",
      "115199/115199 [==============================] - 11s - loss: 0.5656 - acc: 0.7095 - val_loss: 0.5738 - val_acc: 0.7011\n",
      "Epoch 32/50\n",
      "115199/115199 [==============================] - 12s - loss: 0.5643 - acc: 0.7101 - val_loss: 0.5805 - val_acc: 0.6941\n",
      "Epoch 33/50\n",
      "115199/115199 [==============================] - 15s - loss: 0.5629 - acc: 0.7109 - val_loss: 0.5800 - val_acc: 0.7006\n",
      "Epoch 34/50\n",
      "115199/115199 [==============================] - 14s - loss: 0.5642 - acc: 0.7111 - val_loss: 0.5732 - val_acc: 0.7030\n",
      "Epoch 35/50\n",
      "115199/115199 [==============================] - 12s - loss: 0.5634 - acc: 0.7118 - val_loss: 0.5726 - val_acc: 0.7037\n",
      "Epoch 36/50\n",
      "115199/115199 [==============================] - 14s - loss: 0.5622 - acc: 0.7123 - val_loss: 0.5726 - val_acc: 0.7031\n",
      "Epoch 37/50\n",
      "115199/115199 [==============================] - 13s - loss: 0.5614 - acc: 0.7131 - val_loss: 0.5745 - val_acc: 0.6998\n",
      "Epoch 38/50\n",
      "115199/115199 [==============================] - 13s - loss: 0.5601 - acc: 0.7128 - val_loss: 0.5706 - val_acc: 0.7034\n",
      "Epoch 39/50\n",
      "115199/115199 [==============================] - 13s - loss: 0.5607 - acc: 0.7124 - val_loss: 0.5710 - val_acc: 0.7018\n",
      "Epoch 40/50\n",
      "115199/115199 [==============================] - 13s - loss: 0.5606 - acc: 0.7131 - val_loss: 0.5688 - val_acc: 0.7065\n",
      "Epoch 41/50\n",
      "115199/115199 [==============================] - 12s - loss: 0.5590 - acc: 0.7146 - val_loss: 0.5720 - val_acc: 0.7014\n",
      "Epoch 42/50\n",
      "115199/115199 [==============================] - 12s - loss: 0.5577 - acc: 0.7149 - val_loss: 0.5749 - val_acc: 0.7049\n",
      "Epoch 43/50\n",
      "115199/115199 [==============================] - 13s - loss: 0.5578 - acc: 0.7159 - val_loss: 0.5693 - val_acc: 0.7064\n",
      "Epoch 44/50\n",
      "115199/115199 [==============================] - 15s - loss: 0.5583 - acc: 0.7152 - val_loss: 0.5691 - val_acc: 0.7069\n",
      "Epoch 45/50\n",
      "115199/115199 [==============================] - 13s - loss: 0.5577 - acc: 0.7152 - val_loss: 0.5676 - val_acc: 0.7075\n",
      "Epoch 46/50\n",
      "115199/115199 [==============================] - 12s - loss: 0.5564 - acc: 0.7174 - val_loss: 0.5669 - val_acc: 0.7078\n",
      "Epoch 47/50\n",
      "115199/115199 [==============================] - 12s - loss: 0.5570 - acc: 0.7165 - val_loss: 0.5676 - val_acc: 0.7082\n",
      "Epoch 48/50\n",
      "115199/115199 [==============================] - 12s - loss: 0.5552 - acc: 0.7181 - val_loss: 0.5664 - val_acc: 0.7088\n",
      "Epoch 49/50\n",
      "115199/115199 [==============================] - 14s - loss: 0.5564 - acc: 0.7168 - val_loss: 0.5665 - val_acc: 0.7065\n",
      "Epoch 50/50\n",
      "115199/115199 [==============================] - 16s - loss: 0.5538 - acc: 0.7183 - val_loss: 0.5677 - val_acc: 0.7065\n",
      "16000/16001 [============================>.] - ETA: 0sTrain on 115199 samples, validate on 28800 samples\n",
      "Epoch 1/50\n",
      "115199/115199 [==============================] - 13s - loss: 0.7058 - acc: 0.5557 - val_loss: 0.6685 - val_acc: 0.5997\n",
      "Epoch 2/50\n",
      "115199/115199 [==============================] - 12s - loss: 0.6561 - acc: 0.6169 - val_loss: 0.6394 - val_acc: 0.6428\n",
      "Epoch 3/50\n",
      "115199/115199 [==============================] - 12s - loss: 0.6356 - acc: 0.6460 - val_loss: 0.6251 - val_acc: 0.6587\n",
      "Epoch 4/50\n",
      "115199/115199 [==============================] - 12s - loss: 0.6220 - acc: 0.6618 - val_loss: 0.6153 - val_acc: 0.6652\n",
      "Epoch 5/50\n",
      "115199/115199 [==============================] - 12s - loss: 0.6133 - acc: 0.6703 - val_loss: 0.6070 - val_acc: 0.6735\n",
      "Epoch 6/50\n",
      "115199/115199 [==============================] - 13s - loss: 0.6072 - acc: 0.6749 - val_loss: 0.6032 - val_acc: 0.6778\n",
      "Epoch 7/50\n",
      "115199/115199 [==============================] - 12s - loss: 0.6005 - acc: 0.6810 - val_loss: 0.6047 - val_acc: 0.6776\n",
      "Epoch 8/50\n",
      "115199/115199 [==============================] - 14s - loss: 0.5988 - acc: 0.6825 - val_loss: 0.5963 - val_acc: 0.6837\n",
      "Epoch 9/50\n",
      "115199/115199 [==============================] - 15s - loss: 0.5952 - acc: 0.6854 - val_loss: 0.5935 - val_acc: 0.6855\n",
      "Epoch 10/50\n",
      "115199/115199 [==============================] - 14s - loss: 0.5925 - acc: 0.6868 - val_loss: 0.5952 - val_acc: 0.6862\n",
      "Epoch 11/50\n",
      "115199/115199 [==============================] - 12s - loss: 0.5887 - acc: 0.6910 - val_loss: 0.5910 - val_acc: 0.6882\n",
      "Epoch 12/50\n",
      "115199/115199 [==============================] - 12s - loss: 0.5875 - acc: 0.6917 - val_loss: 0.5938 - val_acc: 0.6870\n",
      "Epoch 13/50\n",
      "115199/115199 [==============================] - 11s - loss: 0.5845 - acc: 0.6934 - val_loss: 0.5880 - val_acc: 0.6906\n",
      "Epoch 14/50\n",
      "115199/115199 [==============================] - 11s - loss: 0.5842 - acc: 0.6947 - val_loss: 0.5878 - val_acc: 0.6906\n",
      "Epoch 15/50\n",
      "115199/115199 [==============================] - 10s - loss: 0.5808 - acc: 0.6973 - val_loss: 0.5924 - val_acc: 0.6924\n",
      "Epoch 16/50\n",
      "115199/115199 [==============================] - 11s - loss: 0.5799 - acc: 0.6990 - val_loss: 0.5921 - val_acc: 0.6926\n",
      "Epoch 17/50\n",
      "115199/115199 [==============================] - 10s - loss: 0.5790 - acc: 0.6992 - val_loss: 0.5845 - val_acc: 0.6973\n",
      "Epoch 18/50\n",
      "115199/115199 [==============================] - 11s - loss: 0.5783 - acc: 0.6994 - val_loss: 0.5799 - val_acc: 0.6972\n",
      "Epoch 19/50\n",
      "115199/115199 [==============================] - 10s - loss: 0.5760 - acc: 0.7009 - val_loss: 0.5799 - val_acc: 0.6975\n",
      "Epoch 20/50\n",
      "115199/115199 [==============================] - 11s - loss: 0.5759 - acc: 0.7017 - val_loss: 0.5797 - val_acc: 0.6982\n",
      "Epoch 21/50\n",
      "115199/115199 [==============================] - 10s - loss: 0.5722 - acc: 0.7048 - val_loss: 0.5785 - val_acc: 0.7014\n",
      "Epoch 22/50\n",
      "115199/115199 [==============================] - 11s - loss: 0.5733 - acc: 0.7036 - val_loss: 0.5795 - val_acc: 0.6980\n",
      "Epoch 23/50\n",
      "115199/115199 [==============================] - 11s - loss: 0.5721 - acc: 0.7044 - val_loss: 0.5768 - val_acc: 0.7011\n",
      "Epoch 24/50\n",
      "115199/115199 [==============================] - 11s - loss: 0.5708 - acc: 0.7058 - val_loss: 0.5748 - val_acc: 0.7036\n",
      "Epoch 25/50\n",
      "115199/115199 [==============================] - 11s - loss: 0.5709 - acc: 0.7050 - val_loss: 0.5740 - val_acc: 0.7042\n",
      "Epoch 26/50\n",
      "115199/115199 [==============================] - 16s - loss: 0.5672 - acc: 0.7079 - val_loss: 0.5810 - val_acc: 0.6966\n",
      "Epoch 27/50\n",
      "115199/115199 [==============================] - 17s - loss: 0.5686 - acc: 0.7070 - val_loss: 0.5750 - val_acc: 0.7021\n",
      "Epoch 28/50\n",
      "115199/115199 [==============================] - 14s - loss: 0.5678 - acc: 0.7073 - val_loss: 0.5723 - val_acc: 0.7062\n",
      "Epoch 29/50\n",
      "115199/115199 [==============================] - 12s - loss: 0.5666 - acc: 0.7086 - val_loss: 0.5718 - val_acc: 0.7055\n",
      "Epoch 30/50\n",
      "115199/115199 [==============================] - 12s - loss: 0.5661 - acc: 0.7093 - val_loss: 0.5709 - val_acc: 0.7076\n",
      "Epoch 31/50\n",
      "115199/115199 [==============================] - 12s - loss: 0.5630 - acc: 0.7118 - val_loss: 0.5877 - val_acc: 0.6908\n",
      "Epoch 32/50\n",
      "115199/115199 [==============================] - 11s - loss: 0.5646 - acc: 0.7093 - val_loss: 0.5709 - val_acc: 0.7055\n",
      "Epoch 33/50\n",
      "115199/115199 [==============================] - 11s - loss: 0.5630 - acc: 0.7107 - val_loss: 0.5690 - val_acc: 0.7084\n",
      "Epoch 34/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "115199/115199 [==============================] - 12s - loss: 0.5644 - acc: 0.7108 - val_loss: 0.5692 - val_acc: 0.7080\n",
      "Epoch 35/50\n",
      "115199/115199 [==============================] - 12s - loss: 0.5606 - acc: 0.7131 - val_loss: 0.5757 - val_acc: 0.7010\n",
      "Epoch 36/50\n",
      "115199/115199 [==============================] - 12s - loss: 0.5605 - acc: 0.7134 - val_loss: 0.5716 - val_acc: 0.7110\n",
      "Epoch 37/50\n",
      "115199/115199 [==============================] - 12s - loss: 0.5619 - acc: 0.7124 - val_loss: 0.5676 - val_acc: 0.7090\n",
      "Epoch 38/50\n",
      "115199/115199 [==============================] - 13s - loss: 0.5601 - acc: 0.7144 - val_loss: 0.5693 - val_acc: 0.7065\n",
      "Epoch 39/50\n",
      "115199/115199 [==============================] - 13s - loss: 0.5588 - acc: 0.7144 - val_loss: 0.5663 - val_acc: 0.7097\n",
      "Epoch 40/50\n",
      "115199/115199 [==============================] - 12s - loss: 0.5602 - acc: 0.7128 - val_loss: 0.5661 - val_acc: 0.7098\n",
      "Epoch 41/50\n",
      "115199/115199 [==============================] - 11s - loss: 0.5579 - acc: 0.7138 - val_loss: 0.5696 - val_acc: 0.7059\n",
      "Epoch 42/50\n",
      "115199/115199 [==============================] - 11s - loss: 0.5585 - acc: 0.7146 - val_loss: 0.5662 - val_acc: 0.7094\n",
      "Epoch 00041: early stopping\n",
      "15488/16001 [============================>.] - ETA: 0sTrain on 115200 samples, validate on 28800 samples\n",
      "Epoch 1/50\n",
      "115200/115200 [==============================] - 10s - loss: 0.7065 - acc: 0.5372 - val_loss: 0.6766 - val_acc: 0.5787\n",
      "Epoch 2/50\n",
      "115200/115200 [==============================] - 11s - loss: 0.6642 - acc: 0.6027 - val_loss: 0.6501 - val_acc: 0.6210\n",
      "Epoch 3/50\n",
      "115200/115200 [==============================] - 11s - loss: 0.6406 - acc: 0.6355 - val_loss: 0.6303 - val_acc: 0.6484\n",
      "Epoch 4/50\n",
      "115200/115200 [==============================] - 11s - loss: 0.6261 - acc: 0.6553 - val_loss: 0.6186 - val_acc: 0.6633\n",
      "Epoch 5/50\n",
      "115200/115200 [==============================] - 11s - loss: 0.6160 - acc: 0.6657 - val_loss: 0.6114 - val_acc: 0.6710\n",
      "Epoch 6/50\n",
      "115200/115200 [==============================] - 11s - loss: 0.6093 - acc: 0.6726 - val_loss: 0.6053 - val_acc: 0.6761\n",
      "Epoch 7/50\n",
      "115200/115200 [==============================] - 11s - loss: 0.6041 - acc: 0.6782 - val_loss: 0.6019 - val_acc: 0.6792\n",
      "Epoch 8/50\n",
      "115200/115200 [==============================] - 13s - loss: 0.5999 - acc: 0.6811 - val_loss: 0.5989 - val_acc: 0.6823\n",
      "Epoch 9/50\n",
      "115200/115200 [==============================] - 13s - loss: 0.5974 - acc: 0.6835 - val_loss: 0.5946 - val_acc: 0.6863\n",
      "Epoch 10/50\n",
      "115200/115200 [==============================] - 13s - loss: 0.5932 - acc: 0.6882 - val_loss: 0.5934 - val_acc: 0.6850\n",
      "Epoch 11/50\n",
      "115200/115200 [==============================] - 16s - loss: 0.5914 - acc: 0.6899 - val_loss: 0.5912 - val_acc: 0.6876\n",
      "Epoch 12/50\n",
      "115200/115200 [==============================] - 14s - loss: 0.5897 - acc: 0.6904 - val_loss: 0.5873 - val_acc: 0.6914\n",
      "Epoch 13/50\n",
      "115200/115200 [==============================] - 17s - loss: 0.5860 - acc: 0.6940 - val_loss: 0.5909 - val_acc: 0.6866\n",
      "Epoch 14/50\n",
      "115200/115200 [==============================] - 14s - loss: 0.5849 - acc: 0.6952 - val_loss: 0.5862 - val_acc: 0.6902\n",
      "Epoch 15/50\n",
      "115200/115200 [==============================] - 14s - loss: 0.5829 - acc: 0.6960 - val_loss: 0.5861 - val_acc: 0.6898\n",
      "Epoch 16/50\n",
      "115200/115200 [==============================] - 19s - loss: 0.5821 - acc: 0.6971 - val_loss: 0.5813 - val_acc: 0.6952\n",
      "Epoch 17/50\n",
      "115200/115200 [==============================] - 15s - loss: 0.5805 - acc: 0.6982 - val_loss: 0.5801 - val_acc: 0.6959\n",
      "Epoch 18/50\n",
      "115200/115200 [==============================] - 14s - loss: 0.5784 - acc: 0.6999 - val_loss: 0.5809 - val_acc: 0.6948\n",
      "Epoch 19/50\n",
      "115200/115200 [==============================] - 16s - loss: 0.5777 - acc: 0.7014 - val_loss: 0.5783 - val_acc: 0.6969\n",
      "Epoch 20/50\n",
      "115200/115200 [==============================] - 12s - loss: 0.5765 - acc: 0.7036 - val_loss: 0.5800 - val_acc: 0.6944\n",
      "Epoch 21/50\n",
      "115200/115200 [==============================] - 11s - loss: 0.5746 - acc: 0.7038 - val_loss: 0.5858 - val_acc: 0.6905\n",
      "Epoch 22/50\n",
      "115200/115200 [==============================] - 11s - loss: 0.5752 - acc: 0.7031 - val_loss: 0.5795 - val_acc: 0.6949\n",
      "Epoch 23/50\n",
      "115200/115200 [==============================] - 11s - loss: 0.5727 - acc: 0.7060 - val_loss: 0.5764 - val_acc: 0.6978\n",
      "Epoch 24/50\n",
      "115200/115200 [==============================] - 11s - loss: 0.5733 - acc: 0.7054 - val_loss: 0.5756 - val_acc: 0.6984\n",
      "Epoch 25/50\n",
      "115200/115200 [==============================] - 11s - loss: 0.5723 - acc: 0.7060 - val_loss: 0.5754 - val_acc: 0.6987\n",
      "Epoch 26/50\n",
      "115200/115200 [==============================] - 11s - loss: 0.5691 - acc: 0.7078 - val_loss: 0.5746 - val_acc: 0.7019\n",
      "Epoch 27/50\n",
      "115200/115200 [==============================] - 11s - loss: 0.5709 - acc: 0.7068 - val_loss: 0.5717 - val_acc: 0.7019\n",
      "Epoch 28/50\n",
      "115200/115200 [==============================] - 11s - loss: 0.5694 - acc: 0.7077 - val_loss: 0.5716 - val_acc: 0.7009\n",
      "Epoch 29/50\n",
      "115200/115200 [==============================] - 11s - loss: 0.5689 - acc: 0.7083 - val_loss: 0.5705 - val_acc: 0.7025\n",
      "Epoch 30/50\n",
      "115200/115200 [==============================] - 12s - loss: 0.5676 - acc: 0.7093 - val_loss: 0.5702 - val_acc: 0.7019\n",
      "Epoch 31/50\n",
      "115200/115200 [==============================] - 12s - loss: 0.5658 - acc: 0.7105 - val_loss: 0.5703 - val_acc: 0.7041\n",
      "Epoch 32/50\n",
      "115200/115200 [==============================] - 12s - loss: 0.5663 - acc: 0.7098 - val_loss: 0.5700 - val_acc: 0.7046\n",
      "Epoch 33/50\n",
      "115200/115200 [==============================] - 12s - loss: 0.5657 - acc: 0.7110 - val_loss: 0.5682 - val_acc: 0.7049\n",
      "Epoch 34/50\n",
      "115200/115200 [==============================] - 12s - loss: 0.5651 - acc: 0.7114 - val_loss: 0.5700 - val_acc: 0.7041\n",
      "Epoch 35/50\n",
      "115200/115200 [==============================] - 16s - loss: 0.5627 - acc: 0.7130 - val_loss: 0.5677 - val_acc: 0.7052\n",
      "Epoch 36/50\n",
      "115200/115200 [==============================] - 15s - loss: 0.5645 - acc: 0.7119 - val_loss: 0.5677 - val_acc: 0.7064\n",
      "Epoch 37/50\n",
      "115200/115200 [==============================] - 15s - loss: 0.5647 - acc: 0.7112 - val_loss: 0.5680 - val_acc: 0.7061\n",
      "Epoch 38/50\n",
      "115200/115200 [==============================] - 13s - loss: 0.5619 - acc: 0.7144 - val_loss: 0.5721 - val_acc: 0.7020\n",
      "Epoch 39/50\n",
      "115200/115200 [==============================] - 12s - loss: 0.5598 - acc: 0.7148 - val_loss: 0.5654 - val_acc: 0.7071\n",
      "Epoch 40/50\n",
      "115200/115200 [==============================] - 12s - loss: 0.5618 - acc: 0.7145 - val_loss: 0.5665 - val_acc: 0.7071\n",
      "Epoch 41/50\n",
      "115200/115200 [==============================] - 12s - loss: 0.5608 - acc: 0.7150 - val_loss: 0.5657 - val_acc: 0.7083\n",
      "Epoch 42/50\n",
      "115200/115200 [==============================] - 11s - loss: 0.5596 - acc: 0.7157 - val_loss: 0.5647 - val_acc: 0.7088\n",
      "Epoch 43/50\n",
      "115200/115200 [==============================] - 12s - loss: 0.5604 - acc: 0.7136 - val_loss: 0.5637 - val_acc: 0.7088\n",
      "Epoch 44/50\n",
      "115200/115200 [==============================] - 14s - loss: 0.5592 - acc: 0.7163 - val_loss: 0.5666 - val_acc: 0.7081\n",
      "Epoch 45/50\n",
      "115200/115200 [==============================] - 19s - loss: 0.5585 - acc: 0.7157 - val_loss: 0.5669 - val_acc: 0.7078\n",
      "Epoch 46/50\n",
      "115200/115200 [==============================] - 18s - loss: 0.5564 - acc: 0.7177 - val_loss: 0.5753 - val_acc: 0.7054\n",
      "Epoch 47/50\n",
      "115200/115200 [==============================] - 15s - loss: 0.5594 - acc: 0.7152 - val_loss: 0.5641 - val_acc: 0.7089\n",
      "Epoch 00046: early stopping\n",
      "16000/16000 [==============================] - 1s     \n",
      "Train on 115200 samples, validate on 28800 samples\n",
      "Epoch 1/50\n",
      "115200/115200 [==============================] - 16s - loss: 0.7007 - acc: 0.5434 - val_loss: 0.6728 - val_acc: 0.5923\n",
      "Epoch 2/50\n",
      "115200/115200 [==============================] - 13s - loss: 0.6559 - acc: 0.6138 - val_loss: 0.6456 - val_acc: 0.6307\n",
      "Epoch 3/50\n",
      "115200/115200 [==============================] - 14s - loss: 0.6328 - acc: 0.6426 - val_loss: 0.6285 - val_acc: 0.6487\n",
      "Epoch 4/50\n",
      "115200/115200 [==============================] - 13s - loss: 0.6203 - acc: 0.6575 - val_loss: 0.6262 - val_acc: 0.6573\n",
      "Epoch 5/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "115200/115200 [==============================] - 13s - loss: 0.6148 - acc: 0.6647 - val_loss: 0.6142 - val_acc: 0.6632\n",
      "Epoch 6/50\n",
      "115200/115200 [==============================] - 13s - loss: 0.6081 - acc: 0.6702 - val_loss: 0.6127 - val_acc: 0.6611\n",
      "Epoch 7/50\n",
      "115200/115200 [==============================] - 14s - loss: 0.6036 - acc: 0.6743 - val_loss: 0.6088 - val_acc: 0.6649\n",
      "Epoch 8/50\n",
      "115200/115200 [==============================] - 13s - loss: 0.6001 - acc: 0.6774 - val_loss: 0.6072 - val_acc: 0.6672\n",
      "Epoch 9/50\n",
      "115200/115200 [==============================] - 12s - loss: 0.5969 - acc: 0.6811 - val_loss: 0.6001 - val_acc: 0.6808\n",
      "Epoch 10/50\n",
      "115200/115200 [==============================] - 11s - loss: 0.5945 - acc: 0.6842 - val_loss: 0.5985 - val_acc: 0.6788\n",
      "Epoch 11/50\n",
      "115200/115200 [==============================] - 14s - loss: 0.5921 - acc: 0.6870 - val_loss: 0.5974 - val_acc: 0.6811\n",
      "Epoch 12/50\n",
      "115200/115200 [==============================] - 15s - loss: 0.5889 - acc: 0.6888 - val_loss: 0.6027 - val_acc: 0.6824\n",
      "Epoch 13/50\n",
      "115200/115200 [==============================] - 16s - loss: 0.5871 - acc: 0.6918 - val_loss: 0.5959 - val_acc: 0.6828\n",
      "Epoch 14/50\n",
      "115200/115200 [==============================] - 16s - loss: 0.5863 - acc: 0.6922 - val_loss: 0.5914 - val_acc: 0.6872\n",
      "Epoch 15/50\n",
      "115200/115200 [==============================] - 17s - loss: 0.5834 - acc: 0.6951 - val_loss: 0.5893 - val_acc: 0.6905\n",
      "Epoch 16/50\n",
      "115200/115200 [==============================] - 15s - loss: 0.5826 - acc: 0.6962 - val_loss: 0.5881 - val_acc: 0.6907\n",
      "Epoch 17/50\n",
      "115200/115200 [==============================] - 13s - loss: 0.5804 - acc: 0.6980 - val_loss: 0.5885 - val_acc: 0.6899\n",
      "Epoch 18/50\n",
      "115200/115200 [==============================] - 11s - loss: 0.5808 - acc: 0.6982 - val_loss: 0.5865 - val_acc: 0.6943\n",
      "Epoch 19/50\n",
      "115200/115200 [==============================] - 11s - loss: 0.5778 - acc: 0.7002 - val_loss: 0.5842 - val_acc: 0.6951\n",
      "Epoch 20/50\n",
      "115200/115200 [==============================] - 11s - loss: 0.5776 - acc: 0.7009 - val_loss: 0.5830 - val_acc: 0.6950\n",
      "Epoch 21/50\n",
      "115200/115200 [==============================] - 11s - loss: 0.5762 - acc: 0.7013 - val_loss: 0.5821 - val_acc: 0.6968\n",
      "Epoch 22/50\n",
      "115200/115200 [==============================] - 11s - loss: 0.5745 - acc: 0.7036 - val_loss: 0.5823 - val_acc: 0.6974\n",
      "Epoch 23/50\n",
      "115200/115200 [==============================] - 10s - loss: 0.5739 - acc: 0.7044 - val_loss: 0.5811 - val_acc: 0.6977\n",
      "Epoch 24/50\n",
      "115200/115200 [==============================] - 11s - loss: 0.5728 - acc: 0.7036 - val_loss: 0.5838 - val_acc: 0.6975\n",
      "Epoch 25/50\n",
      "115200/115200 [==============================] - 10s - loss: 0.5716 - acc: 0.7056 - val_loss: 0.5848 - val_acc: 0.6980\n",
      "Epoch 26/50\n",
      "115200/115200 [==============================] - 11s - loss: 0.5705 - acc: 0.7062 - val_loss: 0.5781 - val_acc: 0.6991\n",
      "Epoch 27/50\n",
      "115200/115200 [==============================] - 11s - loss: 0.5706 - acc: 0.7060 - val_loss: 0.5776 - val_acc: 0.6993\n",
      "Epoch 28/50\n",
      "115200/115200 [==============================] - 14s - loss: 0.5700 - acc: 0.7063 - val_loss: 0.5763 - val_acc: 0.7002\n",
      "Epoch 29/50\n",
      "115200/115200 [==============================] - 18s - loss: 0.5679 - acc: 0.7094 - val_loss: 0.5801 - val_acc: 0.6943\n",
      "Epoch 30/50\n",
      "115200/115200 [==============================] - 21s - loss: 0.5670 - acc: 0.7091 - val_loss: 0.5757 - val_acc: 0.7006\n",
      "Epoch 31/50\n",
      "115200/115200 [==============================] - 17s - loss: 0.5675 - acc: 0.7086 - val_loss: 0.5769 - val_acc: 0.6981\n",
      "Epoch 32/50\n",
      "115200/115200 [==============================] - 15s - loss: 0.5649 - acc: 0.7110 - val_loss: 0.5788 - val_acc: 0.6954\n",
      "Epoch 33/50\n",
      "115200/115200 [==============================] - 14s - loss: 0.5648 - acc: 0.7114 - val_loss: 0.5781 - val_acc: 0.6958\n",
      "Epoch 34/50\n",
      "115200/115200 [==============================] - 12s - loss: 0.5637 - acc: 0.7112 - val_loss: 0.5778 - val_acc: 0.6969\n",
      "Epoch 00033: early stopping\n",
      "15840/16000 [============================>.] - ETA: 0sTrain on 115200 samples, validate on 28801 samples\n",
      "Epoch 1/50\n",
      "115200/115200 [==============================] - 13s - loss: 0.6947 - acc: 0.5575 - val_loss: 0.6625 - val_acc: 0.6099\n",
      "Epoch 2/50\n",
      "115200/115200 [==============================] - 12s - loss: 0.6536 - acc: 0.6197 - val_loss: 0.6380 - val_acc: 0.6453\n",
      "Epoch 3/50\n",
      "115200/115200 [==============================] - 11s - loss: 0.6322 - acc: 0.6466 - val_loss: 0.6229 - val_acc: 0.6628\n",
      "Epoch 4/50\n",
      "115200/115200 [==============================] - 11s - loss: 0.6200 - acc: 0.6612 - val_loss: 0.6164 - val_acc: 0.6659\n",
      "Epoch 5/50\n",
      "115200/115200 [==============================] - 11s - loss: 0.6125 - acc: 0.6694 - val_loss: 0.6073 - val_acc: 0.6738\n",
      "Epoch 6/50\n",
      "115200/115200 [==============================] - 12s - loss: 0.6067 - acc: 0.6744 - val_loss: 0.6018 - val_acc: 0.6834\n",
      "Epoch 7/50\n",
      "115200/115200 [==============================] - 13s - loss: 0.6018 - acc: 0.6809 - val_loss: 0.6009 - val_acc: 0.6860\n",
      "Epoch 8/50\n",
      "115200/115200 [==============================] - 13s - loss: 0.5988 - acc: 0.6839 - val_loss: 0.5949 - val_acc: 0.6889\n",
      "Epoch 9/50\n",
      "115200/115200 [==============================] - 12s - loss: 0.5933 - acc: 0.6874 - val_loss: 0.5944 - val_acc: 0.6924\n",
      "Epoch 10/50\n",
      "115200/115200 [==============================] - 12s - loss: 0.5933 - acc: 0.6879 - val_loss: 0.5927 - val_acc: 0.6934\n",
      "Epoch 11/50\n",
      "115200/115200 [==============================] - 12s - loss: 0.5896 - acc: 0.6909 - val_loss: 0.5886 - val_acc: 0.6948\n",
      "Epoch 12/50\n",
      "115200/115200 [==============================] - 13s - loss: 0.5877 - acc: 0.6934 - val_loss: 0.5863 - val_acc: 0.6962\n",
      "Epoch 13/50\n",
      "115200/115200 [==============================] - 12s - loss: 0.5860 - acc: 0.6944 - val_loss: 0.5846 - val_acc: 0.6987\n",
      "Epoch 14/50\n",
      "115200/115200 [==============================] - 12s - loss: 0.5822 - acc: 0.6976 - val_loss: 0.5883 - val_acc: 0.6975\n",
      "Epoch 15/50\n",
      "115200/115200 [==============================] - 12s - loss: 0.5819 - acc: 0.6983 - val_loss: 0.5829 - val_acc: 0.6990\n",
      "Epoch 16/50\n",
      "115200/115200 [==============================] - 12s - loss: 0.5801 - acc: 0.6993 - val_loss: 0.5867 - val_acc: 0.6979\n",
      "Epoch 17/50\n",
      "115200/115200 [==============================] - 12s - loss: 0.5786 - acc: 0.7003 - val_loss: 0.5790 - val_acc: 0.7024\n",
      "Epoch 18/50\n",
      "115200/115200 [==============================] - 12s - loss: 0.5771 - acc: 0.7022 - val_loss: 0.5784 - val_acc: 0.7028\n",
      "Epoch 19/50\n",
      "115200/115200 [==============================] - 12s - loss: 0.5760 - acc: 0.7035 - val_loss: 0.5776 - val_acc: 0.7033\n",
      "Epoch 20/50\n",
      "115200/115200 [==============================] - 12s - loss: 0.5740 - acc: 0.7047 - val_loss: 0.5790 - val_acc: 0.7025\n",
      "Epoch 21/50\n",
      "115200/115200 [==============================] - 12s - loss: 0.5723 - acc: 0.7061 - val_loss: 0.5774 - val_acc: 0.7025\n",
      "Epoch 22/50\n",
      "115200/115200 [==============================] - 12s - loss: 0.5718 - acc: 0.7069 - val_loss: 0.5792 - val_acc: 0.7030\n",
      "Epoch 23/50\n",
      "115200/115200 [==============================] - 12s - loss: 0.5704 - acc: 0.7082 - val_loss: 0.5761 - val_acc: 0.7033\n",
      "Epoch 00022: early stopping\n",
      "15552/15999 [============================>.] - ETA: 0sTrain on 115200 samples, validate on 28801 samples\n",
      "Epoch 1/50\n",
      "115200/115200 [==============================] - 12s - loss: 0.7102 - acc: 0.5282 - val_loss: 0.6827 - val_acc: 0.5763\n",
      "Epoch 2/50\n",
      "115200/115200 [==============================] - 12s - loss: 0.6672 - acc: 0.5982 - val_loss: 0.6505 - val_acc: 0.6222\n",
      "Epoch 3/50\n",
      "115200/115200 [==============================] - 12s - loss: 0.6400 - acc: 0.6370 - val_loss: 0.6318 - val_acc: 0.6464\n",
      "Epoch 4/50\n",
      "115200/115200 [==============================] - 12s - loss: 0.6238 - acc: 0.6566 - val_loss: 0.6248 - val_acc: 0.6592\n",
      "Epoch 5/50\n",
      "115200/115200 [==============================] - 12s - loss: 0.6145 - acc: 0.6660 - val_loss: 0.6129 - val_acc: 0.6667\n",
      "Epoch 6/50\n",
      "115200/115200 [==============================] - 12s - loss: 0.6074 - acc: 0.6731 - val_loss: 0.6072 - val_acc: 0.6733\n",
      "Epoch 7/50\n",
      "115200/115200 [==============================] - 13s - loss: 0.6031 - acc: 0.6763 - val_loss: 0.6031 - val_acc: 0.6767\n",
      "Epoch 8/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "115200/115200 [==============================] - 12s - loss: 0.5969 - acc: 0.6828 - val_loss: 0.6014 - val_acc: 0.6804\n",
      "Epoch 9/50\n",
      "115200/115200 [==============================] - 12s - loss: 0.5953 - acc: 0.6832 - val_loss: 0.5967 - val_acc: 0.6842\n",
      "Epoch 10/50\n",
      "115200/115200 [==============================] - 12s - loss: 0.5905 - acc: 0.6882 - val_loss: 0.6015 - val_acc: 0.6822\n",
      "Epoch 11/50\n",
      "115200/115200 [==============================] - 12s - loss: 0.5894 - acc: 0.6895 - val_loss: 0.5945 - val_acc: 0.6824\n",
      "Epoch 12/50\n",
      "115200/115200 [==============================] - 12s - loss: 0.5864 - acc: 0.6922 - val_loss: 0.5961 - val_acc: 0.6807\n",
      "Epoch 13/50\n",
      "115200/115200 [==============================] - 11s - loss: 0.5841 - acc: 0.6946 - val_loss: 0.5903 - val_acc: 0.6869\n",
      "Epoch 14/50\n",
      "115200/115200 [==============================] - 12s - loss: 0.5829 - acc: 0.6953 - val_loss: 0.5894 - val_acc: 0.6874\n",
      "Epoch 15/50\n",
      "115200/115200 [==============================] - 12s - loss: 0.5817 - acc: 0.6959 - val_loss: 0.5866 - val_acc: 0.6894\n",
      "Epoch 16/50\n",
      "115200/115200 [==============================] - 12s - loss: 0.5787 - acc: 0.6988 - val_loss: 0.5857 - val_acc: 0.6904\n",
      "Epoch 17/50\n",
      "115200/115200 [==============================] - 12s - loss: 0.5783 - acc: 0.7000 - val_loss: 0.5835 - val_acc: 0.6943\n",
      "Epoch 18/50\n",
      "115200/115200 [==============================] - 12s - loss: 0.5750 - acc: 0.7018 - val_loss: 0.5924 - val_acc: 0.6860\n",
      "Epoch 19/50\n",
      "115200/115200 [==============================] - 12s - loss: 0.5756 - acc: 0.7014 - val_loss: 0.5836 - val_acc: 0.6958\n",
      "Epoch 20/50\n",
      "115200/115200 [==============================] - 12s - loss: 0.5726 - acc: 0.7043 - val_loss: 0.5845 - val_acc: 0.6965\n",
      "Epoch 21/50\n",
      "115200/115200 [==============================] - 12s - loss: 0.5735 - acc: 0.7035 - val_loss: 0.5835 - val_acc: 0.6971\n",
      "Epoch 22/50\n",
      "115200/115200 [==============================] - 12s - loss: 0.5714 - acc: 0.7051 - val_loss: 0.5795 - val_acc: 0.6967\n",
      "Epoch 23/50\n",
      "115200/115200 [==============================] - 12s - loss: 0.5704 - acc: 0.7060 - val_loss: 0.5797 - val_acc: 0.6969\n",
      "Epoch 24/50\n",
      "115200/115200 [==============================] - 12s - loss: 0.5682 - acc: 0.7075 - val_loss: 0.5817 - val_acc: 0.6955\n",
      "Epoch 25/50\n",
      "115200/115200 [==============================] - 12s - loss: 0.5675 - acc: 0.7087 - val_loss: 0.5822 - val_acc: 0.6977\n",
      "Epoch 26/50\n",
      "115200/115200 [==============================] - 12s - loss: 0.5675 - acc: 0.7090 - val_loss: 0.5758 - val_acc: 0.7016\n",
      "Epoch 27/50\n",
      "115200/115200 [==============================] - 12s - loss: 0.5661 - acc: 0.7107 - val_loss: 0.5746 - val_acc: 0.7014\n",
      "Epoch 28/50\n",
      "115200/115200 [==============================] - 12s - loss: 0.5650 - acc: 0.7101 - val_loss: 0.5764 - val_acc: 0.7018\n",
      "Epoch 29/50\n",
      "115200/115200 [==============================] - 12s - loss: 0.5642 - acc: 0.7119 - val_loss: 0.5739 - val_acc: 0.7024\n",
      "Epoch 30/50\n",
      "115200/115200 [==============================] - 12s - loss: 0.5636 - acc: 0.7124 - val_loss: 0.5727 - val_acc: 0.7032\n",
      "Epoch 31/50\n",
      "115200/115200 [==============================] - 12s - loss: 0.5626 - acc: 0.7123 - val_loss: 0.5718 - val_acc: 0.7036\n",
      "Epoch 32/50\n",
      "115200/115200 [==============================] - 12s - loss: 0.5621 - acc: 0.7132 - val_loss: 0.5712 - val_acc: 0.7046\n",
      "Epoch 33/50\n",
      "115200/115200 [==============================] - 12s - loss: 0.5611 - acc: 0.7136 - val_loss: 0.5706 - val_acc: 0.7049\n",
      "Epoch 34/50\n",
      "115200/115200 [==============================] - 12s - loss: 0.5598 - acc: 0.7150 - val_loss: 0.5735 - val_acc: 0.7027\n",
      "Epoch 35/50\n",
      "115200/115200 [==============================] - 12s - loss: 0.5597 - acc: 0.7149 - val_loss: 0.5720 - val_acc: 0.7043\n",
      "Epoch 36/50\n",
      "115200/115200 [==============================] - 12s - loss: 0.5569 - acc: 0.7167 - val_loss: 0.5704 - val_acc: 0.7057\n",
      "Epoch 37/50\n",
      "115200/115200 [==============================] - 12s - loss: 0.5593 - acc: 0.7157 - val_loss: 0.5705 - val_acc: 0.7061\n",
      "Epoch 38/50\n",
      "115200/115200 [==============================] - 12s - loss: 0.5575 - acc: 0.7167 - val_loss: 0.5722 - val_acc: 0.7058\n",
      "Epoch 39/50\n",
      "115200/115200 [==============================] - 13s - loss: 0.5563 - acc: 0.7176 - val_loss: 0.5707 - val_acc: 0.7067\n",
      "Epoch 40/50\n",
      "115200/115200 [==============================] - 13s - loss: 0.5556 - acc: 0.7175 - val_loss: 0.5676 - val_acc: 0.7095\n",
      "Epoch 41/50\n",
      "115200/115200 [==============================] - 12s - loss: 0.5568 - acc: 0.7176 - val_loss: 0.5673 - val_acc: 0.7082\n",
      "Epoch 42/50\n",
      "115200/115200 [==============================] - 12s - loss: 0.5539 - acc: 0.7189 - val_loss: 0.5670 - val_acc: 0.7096\n",
      "Epoch 43/50\n",
      "115200/115200 [==============================] - 12s - loss: 0.5549 - acc: 0.7189 - val_loss: 0.5684 - val_acc: 0.7080\n",
      "Epoch 44/50\n",
      "115200/115200 [==============================] - 12s - loss: 0.5536 - acc: 0.7196 - val_loss: 0.5703 - val_acc: 0.7074\n",
      "Epoch 45/50\n",
      "115200/115200 [==============================] - 12s - loss: 0.5535 - acc: 0.7194 - val_loss: 0.5672 - val_acc: 0.7095\n",
      "Epoch 46/50\n",
      "115200/115200 [==============================] - 12s - loss: 0.5519 - acc: 0.7212 - val_loss: 0.5761 - val_acc: 0.6999\n",
      "Epoch 00045: early stopping\n",
      "15744/15999 [============================>.] - ETA: 0sTrain on 115200 samples, validate on 28801 samples\n",
      "Epoch 1/50\n",
      "115200/115200 [==============================] - 12s - loss: 0.7222 - acc: 0.5238 - val_loss: 0.6872 - val_acc: 0.5686\n",
      "Epoch 2/50\n",
      "115200/115200 [==============================] - 12s - loss: 0.6727 - acc: 0.5930 - val_loss: 0.6564 - val_acc: 0.6182\n",
      "Epoch 3/50\n",
      "115200/115200 [==============================] - 12s - loss: 0.6455 - acc: 0.6306 - val_loss: 0.6418 - val_acc: 0.6440\n",
      "Epoch 4/50\n",
      "115200/115200 [==============================] - 12s - loss: 0.6320 - acc: 0.6477 - val_loss: 0.6267 - val_acc: 0.6590\n",
      "Epoch 5/50\n",
      "115200/115200 [==============================] - 12s - loss: 0.6203 - acc: 0.6614 - val_loss: 0.6214 - val_acc: 0.6623\n",
      "Epoch 6/50\n",
      "115200/115200 [==============================] - 12s - loss: 0.6137 - acc: 0.6672 - val_loss: 0.6179 - val_acc: 0.6657\n",
      "Epoch 7/50\n",
      "115200/115200 [==============================] - 12s - loss: 0.6091 - acc: 0.6723 - val_loss: 0.6085 - val_acc: 0.6766\n",
      "Epoch 8/50\n",
      "115200/115200 [==============================] - 12s - loss: 0.6035 - acc: 0.6784 - val_loss: 0.6053 - val_acc: 0.6762\n",
      "Epoch 9/50\n",
      "115200/115200 [==============================] - 12s - loss: 0.6012 - acc: 0.6805 - val_loss: 0.6009 - val_acc: 0.6826\n",
      "Epoch 10/50\n",
      "115200/115200 [==============================] - 12s - loss: 0.5977 - acc: 0.6832 - val_loss: 0.5982 - val_acc: 0.6856\n",
      "Epoch 11/50\n",
      "115200/115200 [==============================] - 12s - loss: 0.5949 - acc: 0.6859 - val_loss: 0.5962 - val_acc: 0.6837\n",
      "Epoch 12/50\n",
      "115200/115200 [==============================] - 13s - loss: 0.5910 - acc: 0.6892 - val_loss: 0.5963 - val_acc: 0.6882\n",
      "Epoch 13/50\n",
      "115200/115200 [==============================] - 13s - loss: 0.5909 - acc: 0.6895 - val_loss: 0.5921 - val_acc: 0.6895\n",
      "Epoch 14/50\n",
      "115200/115200 [==============================] - 12s - loss: 0.5887 - acc: 0.6921 - val_loss: 0.5901 - val_acc: 0.6922\n",
      "Epoch 15/50\n",
      "115200/115200 [==============================] - 12s - loss: 0.5856 - acc: 0.6942 - val_loss: 0.6003 - val_acc: 0.6824\n",
      "Epoch 16/50\n",
      "115200/115200 [==============================] - 12s - loss: 0.5855 - acc: 0.6937 - val_loss: 0.5870 - val_acc: 0.6947\n",
      "Epoch 17/50\n",
      "115200/115200 [==============================] - 13s - loss: 0.5839 - acc: 0.6962 - val_loss: 0.5873 - val_acc: 0.6932\n",
      "Epoch 18/50\n",
      "115200/115200 [==============================] - 13s - loss: 0.5806 - acc: 0.6977 - val_loss: 0.5923 - val_acc: 0.6932\n",
      "Epoch 19/50\n",
      "115200/115200 [==============================] - 11s - loss: 0.5803 - acc: 0.6982 - val_loss: 0.5839 - val_acc: 0.6997\n",
      "Epoch 20/50\n",
      "115200/115200 [==============================] - 11s - loss: 0.5795 - acc: 0.6992 - val_loss: 0.5820 - val_acc: 0.7005\n",
      "Epoch 21/50\n",
      "115200/115200 [==============================] - 11s - loss: 0.5783 - acc: 0.6997 - val_loss: 0.5823 - val_acc: 0.7007\n",
      "Epoch 22/50\n",
      "115200/115200 [==============================] - 11s - loss: 0.5771 - acc: 0.7011 - val_loss: 0.5817 - val_acc: 0.7026\n",
      "Epoch 23/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "115200/115200 [==============================] - 11s - loss: 0.5753 - acc: 0.7020 - val_loss: 0.5858 - val_acc: 0.6990\n",
      "Epoch 24/50\n",
      "115200/115200 [==============================] - 11s - loss: 0.5748 - acc: 0.7024 - val_loss: 0.5823 - val_acc: 0.7020\n",
      "Epoch 25/50\n",
      "115200/115200 [==============================] - 11s - loss: 0.5728 - acc: 0.7030 - val_loss: 0.5829 - val_acc: 0.6967\n",
      "Epoch 26/50\n",
      "115200/115200 [==============================] - 11s - loss: 0.5727 - acc: 0.7035 - val_loss: 0.5777 - val_acc: 0.7024\n",
      "Epoch 27/50\n",
      "115200/115200 [==============================] - 11s - loss: 0.5725 - acc: 0.7039 - val_loss: 0.5769 - val_acc: 0.7032\n",
      "Epoch 28/50\n",
      "115200/115200 [==============================] - 11s - loss: 0.5695 - acc: 0.7057 - val_loss: 0.5803 - val_acc: 0.6998\n",
      "Epoch 00027: early stopping\n",
      "15520/15999 [============================>.] - ETA: 0sTrain on 115200 samples, validate on 28801 samples\n",
      "Epoch 1/50\n",
      "115200/115200 [==============================] - 11s - loss: 0.7089 - acc: 0.5274 - val_loss: 0.6741 - val_acc: 0.5841\n",
      "Epoch 2/50\n",
      "115200/115200 [==============================] - 11s - loss: 0.6569 - acc: 0.6160 - val_loss: 0.6436 - val_acc: 0.6328\n",
      "Epoch 3/50\n",
      "115200/115200 [==============================] - 12s - loss: 0.6321 - acc: 0.6445 - val_loss: 0.6267 - val_acc: 0.6492\n",
      "Epoch 4/50\n",
      "115200/115200 [==============================] - 12s - loss: 0.6210 - acc: 0.6559 - val_loss: 0.6209 - val_acc: 0.6581\n",
      "Epoch 5/50\n",
      "115200/115200 [==============================] - 12s - loss: 0.6120 - acc: 0.6647 - val_loss: 0.6146 - val_acc: 0.6647\n",
      "Epoch 6/50\n",
      "115200/115200 [==============================] - 12s - loss: 0.6072 - acc: 0.6713 - val_loss: 0.6085 - val_acc: 0.6690\n",
      "Epoch 7/50\n",
      "115200/115200 [==============================] - 12s - loss: 0.6023 - acc: 0.6762 - val_loss: 0.6079 - val_acc: 0.6731\n",
      "Epoch 8/50\n",
      "115200/115200 [==============================] - 12s - loss: 0.5986 - acc: 0.6795 - val_loss: 0.6024 - val_acc: 0.6770\n",
      "Epoch 9/50\n",
      "115200/115200 [==============================] - 12s - loss: 0.5932 - acc: 0.6846 - val_loss: 0.6023 - val_acc: 0.6736\n",
      "Epoch 10/50\n",
      "115200/115200 [==============================] - 14s - loss: 0.5927 - acc: 0.6850 - val_loss: 0.5969 - val_acc: 0.6820\n",
      "Epoch 11/50\n",
      "115200/115200 [==============================] - 16s - loss: 0.5894 - acc: 0.6880 - val_loss: 0.5946 - val_acc: 0.6840\n",
      "Epoch 12/50\n",
      "115200/115200 [==============================] - 14s - loss: 0.5880 - acc: 0.6897 - val_loss: 0.5953 - val_acc: 0.6853\n",
      "Epoch 13/50\n",
      "115200/115200 [==============================] - 13s - loss: 0.5848 - acc: 0.6924 - val_loss: 0.5962 - val_acc: 0.6853\n",
      "Epoch 14/50\n",
      "115200/115200 [==============================] - 12s - loss: 0.5841 - acc: 0.6932 - val_loss: 0.5970 - val_acc: 0.6847\n",
      "Epoch 15/50\n",
      "115200/115200 [==============================] - 12s - loss: 0.5820 - acc: 0.6955 - val_loss: 0.5918 - val_acc: 0.6903\n",
      "Epoch 16/50\n",
      "115200/115200 [==============================] - 12s - loss: 0.5809 - acc: 0.6956 - val_loss: 0.5892 - val_acc: 0.6909\n",
      "Epoch 17/50\n",
      "115200/115200 [==============================] - 12s - loss: 0.5787 - acc: 0.6981 - val_loss: 0.5871 - val_acc: 0.6921\n",
      "Epoch 18/50\n",
      "115200/115200 [==============================] - 12s - loss: 0.5788 - acc: 0.6986 - val_loss: 0.5846 - val_acc: 0.6929\n",
      "Epoch 19/50\n",
      "115200/115200 [==============================] - 12s - loss: 0.5765 - acc: 0.6996 - val_loss: 0.5880 - val_acc: 0.6920\n",
      "Epoch 20/50\n",
      "115200/115200 [==============================] - 12s - loss: 0.5749 - acc: 0.7007 - val_loss: 0.5858 - val_acc: 0.6930\n",
      "Epoch 21/50\n",
      "115200/115200 [==============================] - 13s - loss: 0.5742 - acc: 0.7022 - val_loss: 0.5914 - val_acc: 0.6914\n",
      "Epoch 22/50\n",
      "115200/115200 [==============================] - 16s - loss: 0.5730 - acc: 0.7028 - val_loss: 0.5847 - val_acc: 0.6946\n",
      "Epoch 23/50\n",
      "115200/115200 [==============================] - 14s - loss: 0.5736 - acc: 0.7032 - val_loss: 0.5844 - val_acc: 0.6948\n",
      "Epoch 24/50\n",
      "115200/115200 [==============================] - 13s - loss: 0.5707 - acc: 0.7047 - val_loss: 0.5838 - val_acc: 0.6960\n",
      "Epoch 25/50\n",
      "115200/115200 [==============================] - 11s - loss: 0.5686 - acc: 0.7064 - val_loss: 0.5854 - val_acc: 0.6955\n",
      "Epoch 26/50\n",
      "115200/115200 [==============================] - 11s - loss: 0.5691 - acc: 0.7061 - val_loss: 0.5766 - val_acc: 0.6994\n",
      "Epoch 27/50\n",
      "115200/115200 [==============================] - 12s - loss: 0.5691 - acc: 0.7063 - val_loss: 0.5794 - val_acc: 0.6992\n",
      "Epoch 28/50\n",
      "115200/115200 [==============================] - 12s - loss: 0.5670 - acc: 0.7072 - val_loss: 0.5787 - val_acc: 0.6992\n",
      "Epoch 29/50\n",
      "115200/115200 [==============================] - 11s - loss: 0.5661 - acc: 0.7089 - val_loss: 0.5808 - val_acc: 0.6996\n",
      "Epoch 30/50\n",
      "115200/115200 [==============================] - 11s - loss: 0.5648 - acc: 0.7099 - val_loss: 0.5749 - val_acc: 0.7023\n",
      "Epoch 31/50\n",
      "115200/115200 [==============================] - 11s - loss: 0.5666 - acc: 0.7076 - val_loss: 0.5736 - val_acc: 0.7030\n",
      "Epoch 32/50\n",
      "115200/115200 [==============================] - 11s - loss: 0.5636 - acc: 0.7089 - val_loss: 0.5796 - val_acc: 0.7003\n",
      "Epoch 33/50\n",
      "115200/115200 [==============================] - 11s - loss: 0.5637 - acc: 0.7096 - val_loss: 0.5751 - val_acc: 0.7027\n",
      "Epoch 34/50\n",
      "115200/115200 [==============================] - 11s - loss: 0.5616 - acc: 0.7128 - val_loss: 0.5746 - val_acc: 0.7026\n",
      "Epoch 35/50\n",
      "115200/115200 [==============================] - 11s - loss: 0.5624 - acc: 0.7121 - val_loss: 0.5718 - val_acc: 0.7034\n",
      "Epoch 36/50\n",
      "115200/115200 [==============================] - 11s - loss: 0.5612 - acc: 0.7130 - val_loss: 0.5707 - val_acc: 0.7042\n",
      "Epoch 37/50\n",
      "115200/115200 [==============================] - 11s - loss: 0.5611 - acc: 0.7131 - val_loss: 0.5742 - val_acc: 0.7035\n",
      "Epoch 38/50\n",
      "115200/115200 [==============================] - 11s - loss: 0.5607 - acc: 0.7128 - val_loss: 0.5712 - val_acc: 0.7052\n",
      "Epoch 39/50\n",
      "115200/115200 [==============================] - 11s - loss: 0.5595 - acc: 0.7137 - val_loss: 0.5713 - val_acc: 0.7053\n",
      "Epoch 40/50\n",
      "115200/115200 [==============================] - 13s - loss: 0.5584 - acc: 0.7146 - val_loss: 0.5710 - val_acc: 0.7062\n",
      "Epoch 41/50\n",
      "115200/115200 [==============================] - 15s - loss: 0.5570 - acc: 0.7151 - val_loss: 0.5777 - val_acc: 0.7036\n",
      "Epoch 42/50\n",
      "115200/115200 [==============================] - 13s - loss: 0.5564 - acc: 0.7163 - val_loss: 0.5746 - val_acc: 0.6992\n",
      "Epoch 43/50\n",
      "115200/115200 [==============================] - 14s - loss: 0.5568 - acc: 0.7157 - val_loss: 0.5660 - val_acc: 0.7073\n",
      "Epoch 44/50\n",
      "115200/115200 [==============================] - 12s - loss: 0.5584 - acc: 0.7144 - val_loss: 0.5703 - val_acc: 0.7064\n",
      "Epoch 45/50\n",
      "115200/115200 [==============================] - 13s - loss: 0.5540 - acc: 0.7180 - val_loss: 0.5684 - val_acc: 0.7049\n",
      "Epoch 46/50\n",
      "115200/115200 [==============================] - 12s - loss: 0.5557 - acc: 0.7164 - val_loss: 0.5649 - val_acc: 0.7098\n",
      "Epoch 47/50\n",
      "115200/115200 [==============================] - 11s - loss: 0.5559 - acc: 0.7156 - val_loss: 0.5659 - val_acc: 0.7070\n",
      "Epoch 48/50\n",
      "115200/115200 [==============================] - 12s - loss: 0.5544 - acc: 0.7174 - val_loss: 0.5639 - val_acc: 0.7092\n",
      "Epoch 49/50\n",
      "115200/115200 [==============================] - 11s - loss: 0.5547 - acc: 0.7173 - val_loss: 0.5637 - val_acc: 0.7102\n",
      "Epoch 50/50\n",
      "115200/115200 [==============================] - 11s - loss: 0.5521 - acc: 0.7204 - val_loss: 0.5704 - val_acc: 0.7069\n",
      "15552/15999 [============================>.] - ETA: 0s"
     ]
    }
   ],
   "source": [
    "np.random.seed(7)\n",
    "\n",
    "scores = []\n",
    "models = []\n",
    "histories = []\n",
    "\n",
    "folds = StratifiedKFold(n_splits=10, shuffle = True)\n",
    "for train_index, test_index in folds.split(padded_train, labels_train):\n",
    "    training_X, test_X = padded_train[train_index], padded_train[test_index]\n",
    "    training_y, test_y = labels_train[train_index], labels_train[test_index]\n",
    "    \n",
    "    X_train, X_val, y_train, y_val, = split_data(training_X, training_y)\n",
    "    model, history = train_model(cnn, X_train, X_val, y_train, y_val, epochs)\n",
    "    score = model.evaluate(test_X, test_y)\n",
    "    models.append(model)\n",
    "    histories.append(history)\n",
    "    scores.append(score[1])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Training Accuracy: 0.705850 +/- 0.003950\n"
     ]
    }
   ],
   "source": [
    "avg_acc = np.average(scores)\n",
    "avg_std = np.std(scores)\n",
    "\n",
    "print \"Average Training Accuracy: %f +/- %f\" % (avg_acc, avg_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 32/359 [=>............................] - ETA: 1s"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from scipy import stats\n",
    "\n",
    "def majority(models, X):\n",
    "    preds = []\n",
    "    for model in models:\n",
    "        preds.append(model.predict_classes(X))\n",
    "        \n",
    "    return stats.mode(np.array(preds).reshape(10, -1))[0][0]\n",
    "\n",
    "y_pred = majority(models, padded_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Set Accuracy 0.696378830084\n"
     ]
    }
   ],
   "source": [
    "print \"Test Set Accuracy\", accuracy_score(y_pred, labels_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_history(ys, names, title, ylabel):\n",
    "    plt.clf()\n",
    "    for y, name in zip(ys, names):\n",
    "        for points in y:\n",
    "            x = np.arange(1, len(points) + 1)\n",
    "            plt.plot(x, points, label = name)\n",
    "            plt.title(title)\n",
    "            plt.ylabel(ylabel)\n",
    "            plt.xlabel(\"Epoch\")\n",
    "    plt.savefig(\"./figures/\"+title+\".pdf\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYsAAAEWCAYAAACXGLsWAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nLIwxxgzp/wPK6W8DJSY+ogAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1260f4c10>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ys = [[history.val_acc] for history in histories]\n",
    "names = [[\"Itereation\" + str(i)] for i in xrange(1, 11)]\n",
    "\n",
    "plot_history(ys, names, \"CNN Training History (10 Models)\", \"Accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.66573816156\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "X_transformed = [np.concatenate(x) for x in padded_train]\n",
    "X_test_transformed = [np.concatenate(x) for x in padded_test]\n",
    "\n",
    "logR = LogisticRegression().fit(X_transformed, labels_train)\n",
    "y_pred = logR.predict(X_test_transformed)\n",
    "print accuracy_score(labels_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def save_models(models):\n",
    "    i = 1\n",
    "    for model in models:\n",
    "        model.save('./models/model_'+str(i)+'.h5')\n",
    "        i+=1\n",
    "        \n",
    "save_models(models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
